{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQQ_O9Wmsgxw"
      },
      "source": [
        "# Llama2 & Mistral AI efficient fine-tuning using QLoRA, bnb int4, gradient checkpointing and Xâ€”LLM ðŸ¦–\n",
        "\n",
        "- [Xâ€”LLM Repo](https://github.com/BobaZooba/xllm): main repo of the `xllm` library\n",
        "- [Quickstart](https://github.com/KompleteAI/xllm/tree/docs-v1#quickstart-): basics of `xllm`\n",
        "- [Examples](https://github.com/BobaZooba/xllm/examples): minimal examples of using `xllm`\n",
        "- [Guide](https://github.com/BobaZooba/xllm/blob/main/GUIDE.md): here, we go into detail about everything the library can\n",
        "  do\n",
        "- [Demo project](https://github.com/BobaZooba/xllm-demo): here's a minimal step-by-step example of how to use Xâ€”LLM and fit it\n",
        "  into your own project\n",
        "- [WeatherGPT](https://github.com/BobaZooba/wgpt): this repository features an example of how to utilize the xllm library. Included is a solution for a common type of assessment given to LLM engineers, who typically earn between $120,000 to $140,000 annually\n",
        "- [Shurale](https://github.com/BobaZooba/shurale): project with the finetuned 7B Mistal model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVEUs8X5rTiV"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4jm8Qr2qMuU",
        "outputId": "82f4f3e2-b9bc-40ac-f70d-1d7fb9d35bce"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade xllm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_xO5kdESEUl"
      },
      "source": [
        "# Login to HuggingFace to save model to the hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "c_BRWCP9FHnG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /home/louis/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(\"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\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksKCydbdy5Dp"
      },
      "source": [
        "# [Optional] Login to W&B to save training process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8ZvYGsYXyuaY"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlstam\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/louis/.netrc\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# !wandb login\n",
        "import wandb\n",
        "wandb.login(key=\"cafb097edffe235dc31ef69036075037a7818065\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tiine2-9rVpc"
      },
      "source": [
        "# Prepare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import os\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuBIZNOQqZOX",
        "outputId": "99e6deaa-53e3-4006-8a85-b7f91d60492b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Xâ€”LLM version: 0.1.7\n",
            "Torch version: 2.1.1+cu121\n",
            "Cuda is available: True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import xllm\n",
        "\n",
        "cuda_is_available = torch.cuda.is_available()\n",
        "\n",
        "print(f\"Xâ€”LLM version: {xllm.__version__}\\nTorch version: {torch.__version__}\\nCuda is available: {cuda_is_available}\")\n",
        "assert cuda_is_available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DbDhVyrrqbUe"
      },
      "outputs": [],
      "source": [
        "from xllm import Config\n",
        "from xllm.datasets import GeneralDataset\n",
        "from xllm.experiments import Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cU3jAEAgSVmh"
      },
      "source": [
        "# Prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['title', 'output', 'qid', 'documents', 'question'],\n",
              "        num_rows: 10490\n",
              "    })\n",
              "    eval: Dataset({\n",
              "        features: ['title', 'output', 'qid', 'documents', 'question'],\n",
              "        num_rows: 407\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['title', 'output', 'qid', 'documents', 'question'],\n",
              "        num_rows: 558\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# dataset = load_dataset(\"Anthropic/hh-rlhf\")\n",
        "dataset = load_dataset(\"LsTam/cquae_lrec\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "V5IzE3c8rIZf"
      },
      "outputs": [],
      "source": [
        "# train_data = list()\n",
        "\n",
        "# for sample in dataset[\"train\"]:\n",
        "#     train_data.append({\"text\": sample[\"chosen\"].strip()})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_data(dataset):\n",
        "    data = list()\n",
        "\n",
        "    for sample in dataset:\n",
        "        data.append({\n",
        "            \"text\": (\n",
        "            f\"RÃ©ponds Ã  la question suivante en t'appuyant exclusivement sur le document fourni:\"\n",
        "            f\" {sample['question']} documents: {sample['title']} {' '.join(sample['documents'])}\"\n",
        "            f\"target: {sample['output']}\"\n",
        "            )\n",
        "            })\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def template_mistral(data):\n",
        "#     # return {\n",
        "#     #     \"input\": (\n",
        "#     #         f\"<s>[INST] RÃ©ponds Ã  la question suivante en t'appuyant exclusivement sur le document fourni:\"\n",
        "#     #         f\" {data['question']} documents: {data['title']} {' '.join(data['documents'])}  [/INST]\"\n",
        "#     #         ),\n",
        "#     #     \"target\": data['output'],\n",
        "#     #         }\n",
        "#     return (\n",
        "#             f\"<s>[INST] RÃ©ponds Ã  la question suivante en t'appuyant exclusivement sur le document fourni:\"\n",
        "#             f\" {data['question']} documents: {data['title']} {' '.join(data['documents'])}  [/INST]\"\n",
        "            # )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Xj6dzaRC_1UD"
      },
      "outputs": [],
      "source": [
        "train_dataset = GeneralDataset(data=prepare_data(dataset['train']), separator=\"target: \")\n",
        "eval_dataset = GeneralDataset(data=prepare_data(dataset['eval']), separator=\"target: \")\n",
        "# train_dataset = GeneralDataset.from_list(data=td, separator=\"target: \")\n",
        "# GeneralDataset.from_list(data=train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "({'text_parts': [\"RÃ©ponds Ã  la question suivante en t'appuyant exclusivement sur le document fourni: Comment est perÃ§u The Shard dans le monde ? documents: Londres, une mÃ©tropole de rang mondial Â« The Shard Â», nouveau symbole de la puissance de Londres, a Ã©tÃ© inaugurÃ©e en 2013 sur la rive Sud de la Tamise. Cette tour fait face au quartier dâ€™affaires de la City.\",\n",
              "   'The Shard est perÃ§u dans le monde comme le nouveau symbole de la puissance de Londres.']},\n",
              " {'text_parts': ['RÃ©ponds Ã  la question suivante en t\\'appuyant exclusivement sur le document fourni: Pourquoi le dÃ©sert empruntÃ© par le pÃ¨lerin bouddhiste du VIIe siÃ¨cle aprÃ¨s J.â€‘C., Xuanzang, est appelÃ© \"Fleuve du sable\" ? documents: Les routes de la soie, une aventure !  Xuanzang, pÃ¨lerin bouddhiste du VIIe siÃ¨cle aprÃ¨s\\xa0J.â€‘C., voyage durant dix-neuf ans sur les routes de la soie et en dÃ©crit les dangers.\\n\\n Il entra dans le dÃ©sert que les anciens appelaient Le Fleuve de Sable\\xa0: on n\\'y voit ni oiseau ni animal, ni eau ni pÃ¢turages. [...] Ã€ cette Ã©poque, plusieurs dizaines de marchands Ã©trangers qui voyageaient ensemble partirent secrÃ¨tement pendant la nuit. Ils furent assaillis par des brigands qui les pillÃ¨rent et les tuÃ¨rent.\\n\\nHistoire de la vie de Xuanzang et de ses voyages dans l\\'Inde, VIIe siÃ¨cle aprÃ¨s\\xa0J.â€‘C.',\n",
              "   'Le dÃ©sert empruntÃ© par le pÃ¨lerin bouddhiste Xuanzang, au VIIe siÃ¨cle aprÃ¨s J.â€‘C., a Ã©tÃ© nommÃ©e par les anciens \"Fleuve du sable\" car on n\\'y voit ni oiseau, ni animal, ni eau, ni pÃ¢turages.']})"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset[5], eval_dataset[5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjkDiGjqSX8-"
      },
      "source": [
        "# Make a Xâ€”LLM config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-a3MAkBFSKD_"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2023-12-02 00:40:49.043\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mEnvironment variable WANDB_PROJECT set\u001b[0m\n",
            "\u001b[32m2023-12-02 00:40:49.044\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mEnvironment variable WANDB_ENTITY set\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "config = Config(\n",
        "    collator_key=\"lm\",\n",
        "    use_gradient_checkpointing=True,\n",
        "    # model_name_or_path=\"TinyPixel/Llama-2-7B-bf16-sharded\",\n",
        "    model_name_or_path=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "    use_flash_attention_2=True,  # not supported in colab\n",
        "    load_in_4bit=True,\n",
        "    prepare_model_for_kbit_training=True,\n",
        "    apply_lora=True,\n",
        "    warmup_steps=5,\n",
        "    # max_steps=25,\n",
        "    logging_steps=5,\n",
        "    save_steps=25,\n",
        "    num_train_epochs=2,\n",
        "\n",
        "    device_map={'':0},\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=32,\n",
        "    max_length=1024, #2048, #3072,\n",
        "\n",
        "    # tokenizer_padding_side=\"right\",  # good for llama2\n",
        "\n",
        "    # ATTENTON: set your values\n",
        "    push_to_hub=True,\n",
        "    hub_private_repo=True,\n",
        "    hub_model_id=\"LsTam/mistral-xllm-7B-LoRA\",\n",
        "\n",
        "    # W&B\n",
        "    report_to_wandb=True,\n",
        "    wandb_project=\"xllm-demo\",\n",
        "    wandb_entity=\"mistral-xllm\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcXUNRykSdmG"
      },
      "source": [
        "# Make a Xâ€”LLM experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "gUiDbGwm_1I7"
      },
      "outputs": [],
      "source": [
        "experiment = Experiment(config=config, train_dataset=train_dataset, eval_dataset=eval_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJ0-yJ4ySkC7"
      },
      "source": [
        "## Build experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d88c5c8e210c4da0b9c2ee7f89b389dc",
            "96de69d45978498a9fad654c23241c58",
            "1ed5fde994824de5a57ea2bba1f9624c",
            "577e11275b7944bfb1144f73106e8e53",
            "6fb1d8b513ea49b48983f3790ff4ff15",
            "0957b5a407f34cf49d0618f84bdd10df",
            "4da01b3d4c69442e8e6a03b40095bbb6",
            "cede4624148a42839860d8ea571c7e21",
            "90cf42fa8cf24e98a8f219b805a2c9f9",
            "44b1b854d7a74100a560dff9047989ed",
            "e150f51034f14522a28cfcb95a514592"
          ]
        },
        "id": "SZs_er2r_1DM",
        "outputId": "9a0e0ebd-7d45-42b6-a2c9-06dcc31cd6fc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2023-12-02 00:40:49.059\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mExperiment building has started\u001b[0m\n",
            "\u001b[32m2023-12-02 00:40:49.060\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mConfig:\n",
            "{\n",
            "  \"experiment_key\": \"base\",\n",
            "  \"save_safetensors\": true,\n",
            "  \"max_shard_size\": \"10GB\",\n",
            "  \"local_rank\": 0,\n",
            "  \"use_gradient_checkpointing\": true,\n",
            "  \"trainer_key\": \"lm\",\n",
            "  \"force_fp32\": false,\n",
            "  \"force_fp16\": false,\n",
            "  \"from_gptq\": false,\n",
            "  \"huggingface_hub_token\": null,\n",
            "  \"deepspeed_stage\": 0,\n",
            "  \"deepspeed_config_path\": null,\n",
            "  \"fsdp_strategy\": \"\",\n",
            "  \"fsdp_offload\": true,\n",
            "  \"seed\": 42,\n",
            "  \"stabilize\": false,\n",
            "  \"norm_fp32\": false,\n",
            "  \"path_to_env_file\": \"./.env\",\n",
            "  \"prepare_dataset\": true,\n",
            "  \"lora_hub_model_id\": null,\n",
            "  \"lora_model_local_path\": null,\n",
            "  \"fused_model_local_path\": null,\n",
            "  \"fuse_after_training\": false,\n",
            "  \"quantization_dataset_id\": null,\n",
            "  \"quantization_max_samples\": 1024,\n",
            "  \"quantized_model_path\": \"./quantized_model/\",\n",
            "  \"quantized_hub_model_id\": null,\n",
            "  \"quantized_hub_private_repo\": true,\n",
            "  \"dataset_key\": \"soda\",\n",
            "  \"train_local_path_to_data\": \"./train.jsonl\",\n",
            "  \"eval_local_path_to_data\": null,\n",
            "  \"shuffle\": true,\n",
            "  \"max_eval_samples\": 1000,\n",
            "  \"add_eval_to_train_if_no_path\": false,\n",
            "  \"tokenizer_name_or_path\": null,\n",
            "  \"tokenizer_use_fast\": null,\n",
            "  \"tokenizer_padding_side\": null,\n",
            "  \"collator_key\": \"lm\",\n",
            "  \"max_length\": 1024,\n",
            "  \"model_name_or_path\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
            "  \"push_to_hub_bos_add_bos_token\": false,\n",
            "  \"use_flash_attention_2\": true,\n",
            "  \"trust_remote_code\": false,\n",
            "  \"device_map\": {\n",
            "    \"\": 0\n",
            "  },\n",
            "  \"prepare_model_for_kbit_training\": true,\n",
            "  \"load_in_8bit\": false,\n",
            "  \"load_in_4bit\": true,\n",
            "  \"llm_int8_threshold\": 6.0,\n",
            "  \"llm_int8_has_fp16_weight\": true,\n",
            "  \"bnb_4bit_use_double_quant\": true,\n",
            "  \"bnb_4bit_quant_type\": \"nf4\",\n",
            "  \"bnb_quantize_after_model_init\": false,\n",
            "  \"gptq_bits\": 4,\n",
            "  \"gptq_group_size\": 128,\n",
            "  \"gptq_disable_exllama\": true,\n",
            "  \"apply_lora\": true,\n",
            "  \"lora_rank\": 8,\n",
            "  \"lora_alpha\": 32,\n",
            "  \"lora_dropout\": 0.1,\n",
            "  \"raw_lora_target_modules\": \"all\",\n",
            "  \"output_dir\": \"./outputs/\",\n",
            "  \"per_device_train_batch_size\": 2,\n",
            "  \"do_eval\": false,\n",
            "  \"per_device_eval_batch_size\": null,\n",
            "  \"gradient_accumulation_steps\": 32,\n",
            "  \"eval_accumulation_steps\": null,\n",
            "  \"eval_delay\": 0,\n",
            "  \"eval_steps\": 1000,\n",
            "  \"warmup_steps\": 5,\n",
            "  \"max_steps\": null,\n",
            "  \"num_train_epochs\": 2,\n",
            "  \"learning_rate\": 0.0002,\n",
            "  \"max_grad_norm\": 1.0,\n",
            "  \"weight_decay\": 0.001,\n",
            "  \"label_smoothing_factor\": 0.0,\n",
            "  \"logging_steps\": 5,\n",
            "  \"save_steps\": 25,\n",
            "  \"save_total_limit\": 1,\n",
            "  \"optim\": \"paged_adamw_8bit\",\n",
            "  \"push_to_hub\": true,\n",
            "  \"hub_model_id\": \"LsTam/mistral-xllm-7B-LoRA\",\n",
            "  \"hub_private_repo\": true,\n",
            "  \"neftune_noise_alpha\": null,\n",
            "  \"report_to_wandb\": true,\n",
            "  \"wandb_api_key\": null,\n",
            "  \"wandb_project\": \"xllm-demo\",\n",
            "  \"wandb_entity\": \"mistral-xllm\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2023-12-02 00:40:49.061\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mConfig saved\u001b[0m\n",
            "\u001b[32m2023-12-02 00:40:49.062\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mChecks passed successfully\u001b[0m\n",
            "\u001b[32m2023-12-02 00:40:49.065\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mTraining arguments was built:\n",
            "{\n",
            "  \"output_dir\": \"./outputs/\",\n",
            "  \"overwrite_output_dir\": false,\n",
            "  \"do_train\": false,\n",
            "  \"do_eval\": false,\n",
            "  \"do_predict\": false,\n",
            "  \"evaluation_strategy\": \"no\",\n",
            "  \"prediction_loss_only\": false,\n",
            "  \"per_device_train_batch_size\": 2,\n",
            "  \"per_device_eval_batch_size\": 2,\n",
            "  \"per_gpu_train_batch_size\": null,\n",
            "  \"per_gpu_eval_batch_size\": null,\n",
            "  \"gradient_accumulation_steps\": 32,\n",
            "  \"eval_accumulation_steps\": 32,\n",
            "  \"eval_delay\": 0,\n",
            "  \"learning_rate\": 0.0002,\n",
            "  \"weight_decay\": 0.001,\n",
            "  \"adam_beta1\": 0.9,\n",
            "  \"adam_beta2\": 0.999,\n",
            "  \"adam_epsilon\": 1e-08,\n",
            "  \"max_grad_norm\": 1.0,\n",
            "  \"num_train_epochs\": 2,\n",
            "  \"max_steps\": -1,\n",
            "  \"lr_scheduler_type\": \"linear\",\n",
            "  \"warmup_ratio\": 0.0,\n",
            "  \"warmup_steps\": 5,\n",
            "  \"log_level\": \"info\",\n",
            "  \"log_level_replica\": \"warning\",\n",
            "  \"log_on_each_node\": true,\n",
            "  \"logging_dir\": \"./outputs/runs/Dec02_00-40-49_527835b383d2\",\n",
            "  \"logging_strategy\": \"steps\",\n",
            "  \"logging_first_step\": true,\n",
            "  \"logging_steps\": 5,\n",
            "  \"logging_nan_inf_filter\": true,\n",
            "  \"save_strategy\": \"steps\",\n",
            "  \"save_steps\": 25,\n",
            "  \"save_total_limit\": 1,\n",
            "  \"save_safetensors\": true,\n",
            "  \"save_on_each_node\": false,\n",
            "  \"no_cuda\": false,\n",
            "  \"use_cpu\": false,\n",
            "  \"use_mps_device\": false,\n",
            "  \"seed\": 42,\n",
            "  \"data_seed\": 42,\n",
            "  \"jit_mode_eval\": false,\n",
            "  \"use_ipex\": false,\n",
            "  \"bf16\": true,\n",
            "  \"fp16\": false,\n",
            "  \"fp16_opt_level\": \"O1\",\n",
            "  \"half_precision_backend\": \"auto\",\n",
            "  \"bf16_full_eval\": false,\n",
            "  \"fp16_full_eval\": false,\n",
            "  \"tf32\": null,\n",
            "  \"local_rank\": 0,\n",
            "  \"ddp_backend\": null,\n",
            "  \"tpu_num_cores\": null,\n",
            "  \"tpu_metrics_debug\": false,\n",
            "  \"debug\": [],\n",
            "  \"dataloader_drop_last\": false,\n",
            "  \"eval_steps\": 1000,\n",
            "  \"dataloader_num_workers\": 0,\n",
            "  \"past_index\": -1,\n",
            "  \"run_name\": \"./outputs/\",\n",
            "  \"disable_tqdm\": false,\n",
            "  \"remove_unused_columns\": false,\n",
            "  \"label_names\": null,\n",
            "  \"load_best_model_at_end\": false,\n",
            "  \"metric_for_best_model\": \"loss\",\n",
            "  \"greater_is_better\": false,\n",
            "  \"ignore_data_skip\": false,\n",
            "  \"fsdp\": [],\n",
            "  \"fsdp_min_num_params\": 0,\n",
            "  \"fsdp_config\": {\n",
            "    \"min_num_params\": 0,\n",
            "    \"xla\": false,\n",
            "    \"xla_fsdp_grad_ckpt\": false\n",
            "  },\n",
            "  \"fsdp_transformer_layer_cls_to_wrap\": null,\n",
            "  \"deepspeed\": null,\n",
            "  \"label_smoothing_factor\": 0.0,\n",
            "  \"optim\": \"paged_adamw_8bit\",\n",
            "  \"optim_args\": null,\n",
            "  \"adafactor\": false,\n",
            "  \"group_by_length\": false,\n",
            "  \"length_column_name\": \"length\",\n",
            "  \"report_to\": [\n",
            "    \"wandb\"\n",
            "  ],\n",
            "  \"ddp_find_unused_parameters\": null,\n",
            "  \"ddp_bucket_cap_mb\": null,\n",
            "  \"ddp_broadcast_buffers\": null,\n",
            "  \"dataloader_pin_memory\": true,\n",
            "  \"skip_memory_metrics\": true,\n",
            "  \"use_legacy_prediction_loop\": false,\n",
            "  \"push_to_hub\": true,\n",
            "  \"resume_from_checkpoint\": null,\n",
            "  \"hub_model_id\": \"LsTam/mistral-xllm-7B-LoRA\",\n",
            "  \"hub_strategy\": \"checkpoint\",\n",
            "  \"hub_token\": \"<HUB_TOKEN>\",\n",
            "  \"hub_private_repo\": true,\n",
            "  \"hub_always_push\": false,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"gradient_checkpointing_kwargs\": null,\n",
            "  \"include_inputs_for_metrics\": false,\n",
            "  \"fp16_backend\": \"auto\",\n",
            "  \"push_to_hub_model_id\": null,\n",
            "  \"push_to_hub_organization\": null,\n",
            "  \"push_to_hub_token\": \"<PUSH_TO_HUB_TOKEN>\",\n",
            "  \"mp_parameters\": \"\",\n",
            "  \"auto_find_batch_size\": false,\n",
            "  \"full_determinism\": false,\n",
            "  \"torchdynamo\": null,\n",
            "  \"ray_scope\": \"last\",\n",
            "  \"ddp_timeout\": 1800,\n",
            "  \"torch_compile\": false,\n",
            "  \"torch_compile_backend\": null,\n",
            "  \"torch_compile_mode\": null,\n",
            "  \"dispatch_batches\": null,\n",
            "  \"split_batches\": false,\n",
            "  \"include_tokens_per_second\": false,\n",
            "  \"neftune_noise_alpha\": null\n",
            "}\u001b[0m\n",
            "\u001b[32m2023-12-02 00:40:49.246\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mTokenizer pad token set to eos token\u001b[0m\n",
            "\u001b[32m2023-12-02 00:40:49.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mTokenizer mistralai/Mistral-7B-Instruct-v0.1 was built\u001b[0m\n",
            "\u001b[32m2023-12-02 00:40:49.248\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mCollator LMCollator was built\u001b[0m\n",
            "\u001b[32m2023-12-02 00:40:49.251\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mQuantization config was built:\n",
            "{\n",
            "  \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "  \"bnb_4bit_quant_type\": \"nf4\",\n",
            "  \"bnb_4bit_use_double_quant\": true,\n",
            "  \"llm_int8_has_fp16_weight\": true,\n",
            "  \"load_in_4bit\": true\n",
            "}\n",
            "\u001b[0m\n",
            "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:25<00:00, 12.94s/it]\n",
            "\u001b[32m2023-12-02 00:41:31.422\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mModel prepared for kbit training. Gradient checkpointing: True\u001b[0m\n",
            "\u001b[32m2023-12-02 00:41:31.423\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mModel mistralai/Mistral-7B-Instruct-v0.1 was built\u001b[0m\n",
            "\u001b[32m2023-12-02 00:41:34.802\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mLoRA applied to the model mistralai/Mistral-7B-Instruct-v0.1\u001b[0m\n",
            "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
            "Using auto half precision backend\n",
            "\u001b[32m2023-12-02 00:41:34.922\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mTrainer LMTrainer was built\u001b[0m\n",
            "\u001b[32m2023-12-02 00:41:34.923\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mExperiment built successfully\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "experiment.build()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YYv3yekF_0_8",
        "outputId": "a85ff4ca-4706-4b1c-8ee4-1395a0e32d15"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2023-12-02 00:41:44.560\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mTraining will start soon\u001b[0m\n",
            "***** Running training *****\n",
            "  Num examples = 10,490\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 2\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "  Gradient Accumulation steps = 32\n",
            "  Total optimization steps = 326\n",
            "  Number of trainable parameters = 20,971,520\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/workspace/louist/wandb/run-20231202_004144-47p2w4ub</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lstam/xllm-demo/runs/47p2w4ub' target=\"_blank\">peachy-spaceship-3</a></strong> to <a href='https://wandb.ai/lstam/xllm-demo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/lstam/xllm-demo' target=\"_blank\">https://wandb.ai/lstam/xllm-demo</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/lstam/xllm-demo/runs/47p2w4ub' target=\"_blank\">https://wandb.ai/lstam/xllm-demo/runs/47p2w4ub</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/miniconda3/envs/louis-xllm/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='18' max='326' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 18/326 16:14 < 5:12:43, 0.02 it/s, Epoch 0.10/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.906800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.844000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.597000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.557100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "experiment.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzJvZpySyLvN"
      },
      "source": [
        "# After training steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bP6wy-T8yNm5"
      },
      "outputs": [],
      "source": [
        "# # Fuse LoRA weights\n",
        "# experiment.fuse_lora()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGr6lbtRwgAc"
      },
      "source": [
        "### Or push LoRA weights to HuggingFace Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBwXhM0M_0jx"
      },
      "outputs": [],
      "source": [
        "# # Push to hub\n",
        "# experiment.push_to_hub(\n",
        "#     repo_id=\"BobaZooba/AntModel-7B-XLLM-Demo\",\n",
        "#     private=True,\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl2QiIlGj7r2"
      },
      "source": [
        "# ðŸŽ‰ You are awesome!\n",
        "\n",
        "## Now you know how to prototype models using `xllm`\n",
        "\n",
        "### Explore more examples at Xâ€”LLM repo\n",
        "\n",
        "https://github.com/BobaZooba/xllm\n",
        "\n",
        "Useful materials:\n",
        "\n",
        "- [Xâ€”LLM Repo](https://github.com/BobaZooba/xllm): main repo of the `xllm` library\n",
        "- [Quickstart](https://github.com/KompleteAI/xllm/tree/docs-v1#quickstart-): basics of `xllm`\n",
        "- [Examples](https://github.com/BobaZooba/xllm/examples): minimal examples of using `xllm`\n",
        "- [Guide](https://github.com/BobaZooba/xllm/blob/main/GUIDE.md): here, we go into detail about everything the library can\n",
        "  do\n",
        "- [Demo project](https://github.com/BobaZooba/xllm-demo): here's a minimal step-by-step example of how to use Xâ€”LLM and fit it\n",
        "  into your own project\n",
        "- [WeatherGPT](https://github.com/BobaZooba/wgpt): this repository features an example of how to utilize the xllm library. Included is a solution for a common type of assessment given to LLM engineers, who typically earn between $120,000 to $140,000 annually\n",
        "- [Shurale](https://github.com/BobaZooba/shurale): project with the finetuned 7B Mistal model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oz4LrVcZlE6P"
      },
      "source": [
        "## Tale Quest\n",
        "\n",
        "`Tale Quest` is my personal project which was built using `xllm` and `Shurale`. It's an interactive text-based game\n",
        "in `Telegram` with dynamic AI characters, offering infinite scenarios\n",
        "\n",
        "You will get into exciting journeys and complete fascinating quests. Chat\n",
        "with `George Orwell`, `Tech Entrepreneur`, `Young Wizard`, `Noir Detective`, `Femme Fatale` and many more\n",
        "\n",
        "Try it now: [https://t.me/talequestbot](https://t.me/TaleQuestBot?start=Z2g)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udE7qvGJkUus"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0957b5a407f34cf49d0618f84bdd10df": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ed5fde994824de5a57ea2bba1f9624c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cede4624148a42839860d8ea571c7e21",
            "max": 11,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_90cf42fa8cf24e98a8f219b805a2c9f9",
            "value": 11
          }
        },
        "44b1b854d7a74100a560dff9047989ed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4da01b3d4c69442e8e6a03b40095bbb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "577e11275b7944bfb1144f73106e8e53": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44b1b854d7a74100a560dff9047989ed",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e150f51034f14522a28cfcb95a514592",
            "value": " 11/11 [01:29&lt;00:00,  5.56s/it]"
          }
        },
        "6fb1d8b513ea49b48983f3790ff4ff15": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90cf42fa8cf24e98a8f219b805a2c9f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "96de69d45978498a9fad654c23241c58": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0957b5a407f34cf49d0618f84bdd10df",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4da01b3d4c69442e8e6a03b40095bbb6",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "cede4624148a42839860d8ea571c7e21": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d88c5c8e210c4da0b9c2ee7f89b389dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_96de69d45978498a9fad654c23241c58",
              "IPY_MODEL_1ed5fde994824de5a57ea2bba1f9624c",
              "IPY_MODEL_577e11275b7944bfb1144f73106e8e53"
            ],
            "layout": "IPY_MODEL_6fb1d8b513ea49b48983f3790ff4ff15"
          }
        },
        "e150f51034f14522a28cfcb95a514592": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
