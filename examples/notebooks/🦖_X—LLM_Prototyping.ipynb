{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "collapsed_sections": [
    "187Yhr0Hhs_r"
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 🦖 X—LLM: Easy & Cutting Edge LLM Finetuning\n",
    "\n",
    "Tutorial how to run X—LLM in colab\n",
    "\n",
    "- [X—LLM Repo](https://github.com/BobaZooba/xllm): main repo of the `xllm` library\n",
    "- [Quickstart](https://github.com/KompleteAI/xllm/tree/docs-v1#quickstart-): basics of `xllm`\n",
    "- [Examples](https://github.com/BobaZooba/xllm/examples): minimal examples of using `xllm`\n",
    "- [Guide](https://github.com/BobaZooba/xllm/blob/main/GUIDE.md): here, we go into detail about everything the library can\n",
    "  do\n",
    "- [Demo project](https://github.com/BobaZooba/xllm-demo): here's a minimal step-by-step example of how to use X—LLM and fit it\n",
    "  into your own project\n",
    "- [WeatherGPT](https://github.com/BobaZooba/wgpt): this repository features an example of how to utilize the xllm library. Included is a solution for a common type of assessment given to LLM engineers, who typically earn between $120,000 to $140,000 annually\n",
    "- [Shurale](https://github.com/BobaZooba/shurale): project with the finetuned 7B Mistal model\n"
   ],
   "metadata": {
    "id": "nfE8HHxFECqI"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First of all you need to install the latest `xllm` version"
   ],
   "metadata": {
    "id": "RK7mWDkLEqhX"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Installation"
   ],
   "metadata": {
    "id": "187Yhr0Hhs_r"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7jluomCQ65wT",
    "outputId": "89ec8415-9f01-422e-9597-bc46c2160559"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: xllm in /usr/local/lib/python3.10/dist-packages (0.0.10)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from xllm) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from xllm) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from xllm) (5.9.5)\n",
      "Requirement already satisfied: torch>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from xllm) (2.1.0+cu118)\n",
      "Requirement already satisfied: loguru in /usr/local/lib/python3.10/dist-packages (from xllm) (0.7.2)\n",
      "Requirement already satisfied: peft>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from xllm) (0.6.2)\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (from xllm) (0.16.0)\n",
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from xllm) (1.0.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from xllm) (2.31.0)\n",
      "Requirement already satisfied: optimum>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from xllm) (1.14.0)\n",
      "Requirement already satisfied: bitsandbytes>=0.41.1 in /usr/local/lib/python3.10/dist-packages (from xllm) (0.41.2.post2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xllm) (1.11.3)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from xllm) (4.35.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from xllm) (4.66.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from xllm) (0.4.0)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from optimum>=1.12.0->xllm) (15.0.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from optimum>=1.12.0->xllm) (1.12)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from optimum>=1.12.0->xllm) (0.17.3)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from optimum>=1.12.0->xllm) (2.14.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft>=0.5.0->xllm) (6.0.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft>=0.5.0->xllm) (0.24.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->xllm) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->xllm) (4.5.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->xllm) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->xllm) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->xllm) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->xllm) (2.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->xllm) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->xllm) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->xllm) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->xllm) (2023.7.22)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->xllm) (2023.6.3)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->xllm) (0.14.1)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb->xllm) (8.1.7)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->xllm) (3.1.40)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->xllm) (1.35.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb->xllm) (0.4.0)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb->xllm) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb->xllm) (67.7.2)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb->xllm) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb->xllm) (3.20.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb->xllm) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb->xllm) (4.0.11)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers->xllm) (0.1.99)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->optimum>=1.12.0->xllm) (10.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->optimum>=1.12.0->xllm) (9.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->optimum>=1.12.0->xllm) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->optimum>=1.12.0->xllm) (1.5.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->optimum>=1.12.0->xllm) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->optimum>=1.12.0->xllm) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->optimum>=1.12.0->xllm) (3.8.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.1->xllm) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->optimum>=1.12.0->xllm) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum>=1.12.0->xllm) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum>=1.12.0->xllm) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum>=1.12.0->xllm) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum>=1.12.0->xllm) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum>=1.12.0->xllm) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum>=1.12.0->xllm) (1.3.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->xllm) (5.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->optimum>=1.12.0->xllm) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->optimum>=1.12.0->xllm) (2023.3.post1)\n"
     ]
    }
   ],
   "source": [
    "# default version\n",
    "!pip install xllm\n",
    "\n",
    "# version which include deepspeed, flash-attn and auto-gptq\n",
    "# !pip install xllm[train]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Verify the versions and confirm whether CUDA is available"
   ],
   "metadata": {
    "id": "dOmUEQGPFSPO"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import xllm\n",
    "\n",
    "cuda_is_available = torch.cuda.is_available()\n",
    "\n",
    "print(f\"X—LLM version: {xllm.__version__}\\nTorch version: {torch.__version__}\\nCuda is available: {cuda_is_available}\")\n",
    "assert cuda_is_available"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YpbY13qq7AIr",
    "outputId": "d70b38c7-5f92-4a40-854e-4f4ecd92879a"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X—LLM version: 0.0.10\n",
      "Torch version: 2.1.0+cu118\n",
      "Cuda is available: True\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Single cell example"
   ],
   "metadata": {
    "id": "yX6IKojXS4JH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from xllm import Config\n",
    "from xllm.datasets import GeneralDataset\n",
    "from xllm.experiments import Experiment\n",
    "\n",
    "# 1. Init Config which controls the internal logic of xllm\n",
    "config = Config(\n",
    "    model_name_or_path=\"facebook/opt-350m\",\n",
    "    force_fp32=True,  # only for colab\n",
    ")\n",
    "\n",
    "# 2. Prepare the data\n",
    "train_data = [\"Hello!\"] * 100\n",
    "\n",
    "# 3. Load the data\n",
    "train_dataset = GeneralDataset.from_list(data=train_data)\n",
    "\n",
    "# 4. Init Experiment\n",
    "experiment = Experiment(config=config, train_dataset=train_dataset)\n",
    "\n",
    "# 5. Build Experiment from Config: init tokenizer and model, apply LoRA and so on\n",
    "experiment.build()\n",
    "\n",
    "# 6. Run Experiment (training)\n",
    "experiment.run()\n",
    "\n",
    "# 7. [Optional] Fuse LoRA layers\n",
    "# experiment.fuse_lora()\n",
    "\n",
    "# 8. [Optional] Push fused model (or just LoRA weight) to the HuggingFace Hub\n",
    "# experiment.push_to_hub(repo_id=\"YOUR_NAME/MODEL_NAME\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hS32uIgmdOor",
    "outputId": "5fca53b3-371f-4708-c3be-b9a9244cc2a0"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[32m2023-11-14 15:58:32.074\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mExperiment building has started\u001b[0m\n",
      "\u001b[32m2023-11-14 15:58:32.080\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mConfig:\n",
      "{\n",
      "  \"experiment_key\": \"base\",\n",
      "  \"save_safetensors\": true,\n",
      "  \"max_shard_size\": \"10GB\",\n",
      "  \"local_rank\": 0,\n",
      "  \"use_gradient_checkpointing\": false,\n",
      "  \"trainer_key\": \"lm\",\n",
      "  \"force_fp32\": true,\n",
      "  \"force_fp16\": false,\n",
      "  \"from_gptq\": false,\n",
      "  \"huggingface_hub_token\": null,\n",
      "  \"deepspeed_stage\": 0,\n",
      "  \"deepspeed_config_path\": null,\n",
      "  \"fsdp_strategy\": \"\",\n",
      "  \"fsdp_offload\": true,\n",
      "  \"seed\": 42,\n",
      "  \"stabilize\": false,\n",
      "  \"path_to_env_file\": \"./.env\",\n",
      "  \"prepare_dataset\": true,\n",
      "  \"lora_hub_model_id\": null,\n",
      "  \"lora_model_local_path\": null,\n",
      "  \"fused_model_local_path\": null,\n",
      "  \"fuse_after_training\": false,\n",
      "  \"quantization_dataset_id\": null,\n",
      "  \"quantization_max_samples\": 1024,\n",
      "  \"quantized_model_path\": \"./quantized_model/\",\n",
      "  \"quantized_hub_model_id\": null,\n",
      "  \"quantized_hub_private_repo\": true,\n",
      "  \"dataset_key\": \"soda\",\n",
      "  \"train_local_path_to_data\": \"./train.jsonl\",\n",
      "  \"eval_local_path_to_data\": null,\n",
      "  \"shuffle\": true,\n",
      "  \"max_eval_samples\": 1000,\n",
      "  \"add_eval_to_train_if_no_path\": false,\n",
      "  \"tokenizer_name_or_path\": null,\n",
      "  \"tokenizer_use_fast\": null,\n",
      "  \"tokenizer_padding_side\": null,\n",
      "  \"collator_key\": \"lm\",\n",
      "  \"max_length\": 2048,\n",
      "  \"model_name_or_path\": \"facebook/opt-350m\",\n",
      "  \"push_to_hub_bos_add_bos_token\": false,\n",
      "  \"use_flash_attention_2\": false,\n",
      "  \"trust_remote_code\": false,\n",
      "  \"device_map\": null,\n",
      "  \"prepare_model_for_kbit_training\": true,\n",
      "  \"load_in_8bit\": false,\n",
      "  \"load_in_4bit\": false,\n",
      "  \"llm_int8_threshold\": 6.0,\n",
      "  \"llm_int8_has_fp16_weight\": true,\n",
      "  \"bnb_4bit_use_double_quant\": true,\n",
      "  \"bnb_4bit_quant_type\": \"nf4\",\n",
      "  \"bnb_quantize_after_model_init\": false,\n",
      "  \"gptq_bits\": 4,\n",
      "  \"gptq_group_size\": 128,\n",
      "  \"gptq_disable_exllama\": true,\n",
      "  \"apply_lora\": false,\n",
      "  \"lora_rank\": 8,\n",
      "  \"lora_alpha\": 32,\n",
      "  \"lora_dropout\": 0.1,\n",
      "  \"raw_lora_target_modules\": \"all\",\n",
      "  \"output_dir\": \"./outputs/\",\n",
      "  \"per_device_train_batch_size\": 2,\n",
      "  \"do_eval\": false,\n",
      "  \"per_device_eval_batch_size\": null,\n",
      "  \"gradient_accumulation_steps\": 1,\n",
      "  \"eval_accumulation_steps\": null,\n",
      "  \"eval_delay\": 0,\n",
      "  \"eval_steps\": 1000,\n",
      "  \"warmup_steps\": 1000,\n",
      "  \"max_steps\": null,\n",
      "  \"num_train_epochs\": 1,\n",
      "  \"learning_rate\": 0.0002,\n",
      "  \"max_grad_norm\": 1.0,\n",
      "  \"weight_decay\": 0.001,\n",
      "  \"label_smoothing_factor\": 0.0,\n",
      "  \"logging_steps\": 10,\n",
      "  \"save_steps\": 100,\n",
      "  \"save_total_limit\": 1,\n",
      "  \"optim\": \"paged_adamw_8bit\",\n",
      "  \"push_to_hub\": false,\n",
      "  \"hub_model_id\": null,\n",
      "  \"hub_private_repo\": true,\n",
      "  \"report_to_wandb\": false,\n",
      "  \"wandb_api_key\": null,\n",
      "  \"wandb_project\": null,\n",
      "  \"wandb_entity\": null\n",
      "}\u001b[0m\n",
      "\u001b[32m2023-11-14 15:58:32.083\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mConfig saved\u001b[0m\n",
      "\u001b[32m2023-11-14 15:58:32.095\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mChecks passed successfully\u001b[0m\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "\u001b[32m2023-11-14 15:58:32.114\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mTraining arguments was built:\n",
      "{\n",
      "  \"output_dir\": \"./outputs/\",\n",
      "  \"overwrite_output_dir\": false,\n",
      "  \"do_train\": false,\n",
      "  \"do_eval\": false,\n",
      "  \"do_predict\": false,\n",
      "  \"evaluation_strategy\": \"no\",\n",
      "  \"prediction_loss_only\": false,\n",
      "  \"per_device_train_batch_size\": 2,\n",
      "  \"per_device_eval_batch_size\": 2,\n",
      "  \"per_gpu_train_batch_size\": null,\n",
      "  \"per_gpu_eval_batch_size\": null,\n",
      "  \"gradient_accumulation_steps\": 1,\n",
      "  \"eval_accumulation_steps\": 1,\n",
      "  \"eval_delay\": 0,\n",
      "  \"learning_rate\": 0.0002,\n",
      "  \"weight_decay\": 0.001,\n",
      "  \"adam_beta1\": 0.9,\n",
      "  \"adam_beta2\": 0.999,\n",
      "  \"adam_epsilon\": 1e-08,\n",
      "  \"max_grad_norm\": 1.0,\n",
      "  \"num_train_epochs\": 1,\n",
      "  \"max_steps\": -1,\n",
      "  \"lr_scheduler_type\": \"linear\",\n",
      "  \"warmup_ratio\": 0.0,\n",
      "  \"warmup_steps\": 1000,\n",
      "  \"log_level\": \"info\",\n",
      "  \"log_level_replica\": \"warning\",\n",
      "  \"log_on_each_node\": true,\n",
      "  \"logging_dir\": \"./outputs/runs/Nov14_15-58-32_735f762378cc\",\n",
      "  \"logging_strategy\": \"steps\",\n",
      "  \"logging_first_step\": true,\n",
      "  \"logging_steps\": 10,\n",
      "  \"logging_nan_inf_filter\": true,\n",
      "  \"save_strategy\": \"steps\",\n",
      "  \"save_steps\": 100,\n",
      "  \"save_total_limit\": 1,\n",
      "  \"save_safetensors\": true,\n",
      "  \"save_on_each_node\": false,\n",
      "  \"no_cuda\": false,\n",
      "  \"use_cpu\": false,\n",
      "  \"use_mps_device\": false,\n",
      "  \"seed\": 42,\n",
      "  \"data_seed\": 42,\n",
      "  \"jit_mode_eval\": false,\n",
      "  \"use_ipex\": false,\n",
      "  \"bf16\": false,\n",
      "  \"fp16\": true,\n",
      "  \"fp16_opt_level\": \"O1\",\n",
      "  \"half_precision_backend\": \"auto\",\n",
      "  \"bf16_full_eval\": false,\n",
      "  \"fp16_full_eval\": false,\n",
      "  \"tf32\": null,\n",
      "  \"local_rank\": 0,\n",
      "  \"ddp_backend\": null,\n",
      "  \"tpu_num_cores\": null,\n",
      "  \"tpu_metrics_debug\": false,\n",
      "  \"debug\": [],\n",
      "  \"dataloader_drop_last\": false,\n",
      "  \"eval_steps\": 1000,\n",
      "  \"dataloader_num_workers\": 0,\n",
      "  \"past_index\": -1,\n",
      "  \"run_name\": \"./outputs/\",\n",
      "  \"disable_tqdm\": false,\n",
      "  \"remove_unused_columns\": false,\n",
      "  \"label_names\": null,\n",
      "  \"load_best_model_at_end\": false,\n",
      "  \"metric_for_best_model\": \"loss\",\n",
      "  \"greater_is_better\": false,\n",
      "  \"ignore_data_skip\": false,\n",
      "  \"fsdp\": [],\n",
      "  \"fsdp_min_num_params\": 0,\n",
      "  \"fsdp_config\": {\n",
      "    \"min_num_params\": 0,\n",
      "    \"xla\": false,\n",
      "    \"xla_fsdp_grad_ckpt\": false\n",
      "  },\n",
      "  \"fsdp_transformer_layer_cls_to_wrap\": null,\n",
      "  \"deepspeed\": null,\n",
      "  \"label_smoothing_factor\": 0.0,\n",
      "  \"optim\": \"paged_adamw_8bit\",\n",
      "  \"optim_args\": null,\n",
      "  \"adafactor\": false,\n",
      "  \"group_by_length\": false,\n",
      "  \"length_column_name\": \"length\",\n",
      "  \"report_to\": [\n",
      "    \"tensorboard\"\n",
      "  ],\n",
      "  \"ddp_find_unused_parameters\": null,\n",
      "  \"ddp_bucket_cap_mb\": null,\n",
      "  \"ddp_broadcast_buffers\": null,\n",
      "  \"dataloader_pin_memory\": true,\n",
      "  \"skip_memory_metrics\": true,\n",
      "  \"use_legacy_prediction_loop\": false,\n",
      "  \"push_to_hub\": false,\n",
      "  \"resume_from_checkpoint\": null,\n",
      "  \"hub_model_id\": null,\n",
      "  \"hub_strategy\": \"checkpoint\",\n",
      "  \"hub_token\": \"<HUB_TOKEN>\",\n",
      "  \"hub_private_repo\": true,\n",
      "  \"hub_always_push\": false,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"gradient_checkpointing_kwargs\": null,\n",
      "  \"include_inputs_for_metrics\": false,\n",
      "  \"fp16_backend\": \"auto\",\n",
      "  \"push_to_hub_model_id\": null,\n",
      "  \"push_to_hub_organization\": null,\n",
      "  \"push_to_hub_token\": \"<PUSH_TO_HUB_TOKEN>\",\n",
      "  \"mp_parameters\": \"\",\n",
      "  \"auto_find_batch_size\": false,\n",
      "  \"full_determinism\": false,\n",
      "  \"torchdynamo\": null,\n",
      "  \"ray_scope\": \"last\",\n",
      "  \"ddp_timeout\": 1800,\n",
      "  \"torch_compile\": false,\n",
      "  \"torch_compile_backend\": null,\n",
      "  \"torch_compile_mode\": null,\n",
      "  \"dispatch_batches\": null,\n",
      "  \"split_batches\": false,\n",
      "  \"include_tokens_per_second\": false,\n",
      "  \"neftune_noise_alpha\": null\n",
      "}\u001b[0m\n",
      "\u001b[32m2023-11-14 15:58:32.116\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mEval dataset is None\u001b[0m\n",
      "\u001b[32m2023-11-14 15:58:32.762\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mTokenizer facebook/opt-350m was built\u001b[0m\n",
      "\u001b[32m2023-11-14 15:58:32.769\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mCollator LMCollator was built\u001b[0m\n",
      "\u001b[32m2023-11-14 15:58:32.774\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mQuantization config is None. Model will be loaded using torch.float32\u001b[0m\n",
      "\u001b[32m2023-11-14 15:58:40.542\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mModel facebook/opt-350m was built\u001b[0m\n",
      "Using auto half precision backend\n",
      "\u001b[32m2023-11-14 15:58:45.117\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mTrainer LMTrainer was built\u001b[0m\n",
      "\u001b[32m2023-11-14 15:58:45.120\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mExperiment built successfully\u001b[0m\n",
      "\u001b[32m2023-11-14 15:58:45.122\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mTraining will start soon\u001b[0m\n",
      "***** Running training *****\n",
      "  Num examples = 100\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 50\n",
      "  Number of trainable parameters = 331,196,416\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:19, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.197000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.931000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.759800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.693400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.167400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m2023-11-14 15:59:06.618\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mTraining end\u001b[0m\n",
      "\u001b[32m2023-11-14 15:59:06.622\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mModel saved to ./outputs/\u001b[0m\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Add LoRA"
   ],
   "metadata": {
    "id": "Cr2WobXNrInd"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Config\n",
    "\n",
    "`Config` plays a crucial role in the `xllm` library. It's how we define the workings of the library components, like how to handle data, the methods for training, the type of model to train, and so forth."
   ],
   "metadata": {
    "id": "PJHflQLiFznP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# config with LoRA\n",
    "config = Config(\n",
    "    model_name_or_path=\"facebook/opt-350m\",\n",
    "    stabilize=True,\n",
    "    apply_lora=True,\n",
    ")"
   ],
   "metadata": {
    "id": "5wRPWEC_7ACr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### You can explicitly specify the values of additional parameters in LoRA"
   ],
   "metadata": {
    "id": "qrovyDw2uMzs"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# # extended config with LoRA\n",
    "# config = Config(\n",
    "#     model_name_or_path=\"facebook/opt-350m\",\n",
    "#     stabilize=True,\n",
    "#     apply_lora=True,\n",
    "#     lora_rank=8,\n",
    "#     lora_alpha=32,\n",
    "#     lora_dropout=0.05,\n",
    "#     raw_lora_target_modules=\"all\",\n",
    "# )"
   ],
   "metadata": {
    "id": "PDZSBDkPuLTz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Make training data"
   ],
   "metadata": {
    "id": "vlu2UHTbuLAM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_data = [\"Hello!\", \"How are you?\", \"Are you okay?\"] * 100"
   ],
   "metadata": {
    "id": "1oUYinonrZP-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "len(train_data)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dE8GhcaFHNq4",
    "outputId": "b0a55d10-4db6-4e0a-a04a-38a1f778cdd6"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Make a `xllm` train dataset"
   ],
   "metadata": {
    "id": "LHE8Wxh2IKbV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_dataset = GeneralDataset.from_list(data=train_data)"
   ],
   "metadata": {
    "id": "goJVNKvW6_81"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Init the experiment\n",
    "\n",
    "`Experiment` encompasses all aspects of training, such as how to load the model, whether to use LoRA or not, and how to set up the trainer, among other things.\n",
    "\n",
    "Required field is `config`.\n",
    "\n",
    "You can also pass the arguments that are listed below. Default value for each component is `None`.\n",
    "\n",
    "If you do not explicitly specify the value when initializing the experiment (that is, by default it will be `None`), then `Experiment` in step `.build` initializes the necessary components by referring to `Config` such as `tokenizer`, `model`, and so on.\n",
    "```\n",
    "training_arguments: Optional[TrainingArguments]\n",
    "train_dataset: Optional[BaseDataset]\n",
    "eval_dataset: Optional[BaseDataset]\n",
    "tokenizer: Optional[PreTrainedTokenizer]\n",
    "collator: Optional[BaseCollator]\n",
    "quantization_config: Union[BitsAndBytesConfig, GPTQConfig, None]\n",
    "model: Union[PreTrainedModel, PeftModel, None]\n",
    "lora_config: Optional[LoraConfig]\n",
    "trainer: Optional[LMTrainer]\n",
    "```"
   ],
   "metadata": {
    "id": "FjOvBBY6Iylu"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "experiment = Experiment(config=config, train_dataset=train_dataset)"
   ],
   "metadata": {
    "id": "q051EACF6_6F"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 🏗 Build the experiment\n",
    "\n",
    "At this point, we're setting up all the components needed for training."
   ],
   "metadata": {
    "id": "c_LNu8E4JPIW"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "experiment.build()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A_n-_AZ56_2z",
    "outputId": "2cf97d7a-9cdc-4d15-e3aa-c328fcf65a42"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[32m2023-11-14 15:59:06.695\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mExperiment building has started\u001b[0m\n",
      "\u001b[32m2023-11-14 15:59:06.699\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mConfig:\n",
      "{\n",
      "  \"experiment_key\": \"base\",\n",
      "  \"save_safetensors\": true,\n",
      "  \"max_shard_size\": \"10GB\",\n",
      "  \"local_rank\": 0,\n",
      "  \"use_gradient_checkpointing\": false,\n",
      "  \"trainer_key\": \"lm\",\n",
      "  \"force_fp32\": false,\n",
      "  \"force_fp16\": false,\n",
      "  \"from_gptq\": false,\n",
      "  \"huggingface_hub_token\": null,\n",
      "  \"deepspeed_stage\": 0,\n",
      "  \"deepspeed_config_path\": null,\n",
      "  \"fsdp_strategy\": \"\",\n",
      "  \"fsdp_offload\": true,\n",
      "  \"seed\": 42,\n",
      "  \"stabilize\": true,\n",
      "  \"path_to_env_file\": \"./.env\",\n",
      "  \"prepare_dataset\": true,\n",
      "  \"lora_hub_model_id\": null,\n",
      "  \"lora_model_local_path\": null,\n",
      "  \"fused_model_local_path\": null,\n",
      "  \"fuse_after_training\": false,\n",
      "  \"quantization_dataset_id\": null,\n",
      "  \"quantization_max_samples\": 1024,\n",
      "  \"quantized_model_path\": \"./quantized_model/\",\n",
      "  \"quantized_hub_model_id\": null,\n",
      "  \"quantized_hub_private_repo\": true,\n",
      "  \"dataset_key\": \"soda\",\n",
      "  \"train_local_path_to_data\": \"./train.jsonl\",\n",
      "  \"eval_local_path_to_data\": null,\n",
      "  \"shuffle\": true,\n",
      "  \"max_eval_samples\": 1000,\n",
      "  \"add_eval_to_train_if_no_path\": false,\n",
      "  \"tokenizer_name_or_path\": null,\n",
      "  \"tokenizer_use_fast\": null,\n",
      "  \"tokenizer_padding_side\": null,\n",
      "  \"collator_key\": \"lm\",\n",
      "  \"max_length\": 2048,\n",
      "  \"model_name_or_path\": \"facebook/opt-350m\",\n",
      "  \"push_to_hub_bos_add_bos_token\": false,\n",
      "  \"use_flash_attention_2\": false,\n",
      "  \"trust_remote_code\": false,\n",
      "  \"device_map\": null,\n",
      "  \"prepare_model_for_kbit_training\": true,\n",
      "  \"load_in_8bit\": false,\n",
      "  \"load_in_4bit\": false,\n",
      "  \"llm_int8_threshold\": 6.0,\n",
      "  \"llm_int8_has_fp16_weight\": true,\n",
      "  \"bnb_4bit_use_double_quant\": true,\n",
      "  \"bnb_4bit_quant_type\": \"nf4\",\n",
      "  \"bnb_quantize_after_model_init\": false,\n",
      "  \"gptq_bits\": 4,\n",
      "  \"gptq_group_size\": 128,\n",
      "  \"gptq_disable_exllama\": true,\n",
      "  \"apply_lora\": true,\n",
      "  \"lora_rank\": 8,\n",
      "  \"lora_alpha\": 32,\n",
      "  \"lora_dropout\": 0.1,\n",
      "  \"raw_lora_target_modules\": \"all\",\n",
      "  \"output_dir\": \"./outputs/\",\n",
      "  \"per_device_train_batch_size\": 2,\n",
      "  \"do_eval\": false,\n",
      "  \"per_device_eval_batch_size\": null,\n",
      "  \"gradient_accumulation_steps\": 1,\n",
      "  \"eval_accumulation_steps\": null,\n",
      "  \"eval_delay\": 0,\n",
      "  \"eval_steps\": 1000,\n",
      "  \"warmup_steps\": 1000,\n",
      "  \"max_steps\": null,\n",
      "  \"num_train_epochs\": 1,\n",
      "  \"learning_rate\": 0.0002,\n",
      "  \"max_grad_norm\": 1.0,\n",
      "  \"weight_decay\": 0.001,\n",
      "  \"label_smoothing_factor\": 0.0,\n",
      "  \"logging_steps\": 10,\n",
      "  \"save_steps\": 100,\n",
      "  \"save_total_limit\": 1,\n",
      "  \"optim\": \"paged_adamw_8bit\",\n",
      "  \"push_to_hub\": false,\n",
      "  \"hub_model_id\": null,\n",
      "  \"hub_private_repo\": true,\n",
      "  \"report_to_wandb\": false,\n",
      "  \"wandb_api_key\": null,\n",
      "  \"wandb_project\": null,\n",
      "  \"wandb_entity\": null\n",
      "}\u001b[0m\n",
      "\u001b[32m2023-11-14 15:59:06.700\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mConfig saved\u001b[0m\n",
      "\u001b[32m2023-11-14 15:59:06.709\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mChecks passed successfully\u001b[0m\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "\u001b[32m2023-11-14 15:59:06.722\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mTraining arguments was built:\n",
      "{\n",
      "  \"output_dir\": \"./outputs/\",\n",
      "  \"overwrite_output_dir\": false,\n",
      "  \"do_train\": false,\n",
      "  \"do_eval\": false,\n",
      "  \"do_predict\": false,\n",
      "  \"evaluation_strategy\": \"no\",\n",
      "  \"prediction_loss_only\": false,\n",
      "  \"per_device_train_batch_size\": 2,\n",
      "  \"per_device_eval_batch_size\": 2,\n",
      "  \"per_gpu_train_batch_size\": null,\n",
      "  \"per_gpu_eval_batch_size\": null,\n",
      "  \"gradient_accumulation_steps\": 1,\n",
      "  \"eval_accumulation_steps\": 1,\n",
      "  \"eval_delay\": 0,\n",
      "  \"learning_rate\": 0.0002,\n",
      "  \"weight_decay\": 0.001,\n",
      "  \"adam_beta1\": 0.9,\n",
      "  \"adam_beta2\": 0.999,\n",
      "  \"adam_epsilon\": 1e-08,\n",
      "  \"max_grad_norm\": 1.0,\n",
      "  \"num_train_epochs\": 1,\n",
      "  \"max_steps\": -1,\n",
      "  \"lr_scheduler_type\": \"linear\",\n",
      "  \"warmup_ratio\": 0.0,\n",
      "  \"warmup_steps\": 1000,\n",
      "  \"log_level\": \"info\",\n",
      "  \"log_level_replica\": \"warning\",\n",
      "  \"log_on_each_node\": true,\n",
      "  \"logging_dir\": \"./outputs/runs/Nov14_15-59-06_735f762378cc\",\n",
      "  \"logging_strategy\": \"steps\",\n",
      "  \"logging_first_step\": true,\n",
      "  \"logging_steps\": 10,\n",
      "  \"logging_nan_inf_filter\": true,\n",
      "  \"save_strategy\": \"steps\",\n",
      "  \"save_steps\": 100,\n",
      "  \"save_total_limit\": 1,\n",
      "  \"save_safetensors\": true,\n",
      "  \"save_on_each_node\": false,\n",
      "  \"no_cuda\": false,\n",
      "  \"use_cpu\": false,\n",
      "  \"use_mps_device\": false,\n",
      "  \"seed\": 42,\n",
      "  \"data_seed\": 42,\n",
      "  \"jit_mode_eval\": false,\n",
      "  \"use_ipex\": false,\n",
      "  \"bf16\": false,\n",
      "  \"fp16\": true,\n",
      "  \"fp16_opt_level\": \"O1\",\n",
      "  \"half_precision_backend\": \"auto\",\n",
      "  \"bf16_full_eval\": false,\n",
      "  \"fp16_full_eval\": false,\n",
      "  \"tf32\": null,\n",
      "  \"local_rank\": 0,\n",
      "  \"ddp_backend\": null,\n",
      "  \"tpu_num_cores\": null,\n",
      "  \"tpu_metrics_debug\": false,\n",
      "  \"debug\": [],\n",
      "  \"dataloader_drop_last\": false,\n",
      "  \"eval_steps\": 1000,\n",
      "  \"dataloader_num_workers\": 0,\n",
      "  \"past_index\": -1,\n",
      "  \"run_name\": \"./outputs/\",\n",
      "  \"disable_tqdm\": false,\n",
      "  \"remove_unused_columns\": false,\n",
      "  \"label_names\": null,\n",
      "  \"load_best_model_at_end\": false,\n",
      "  \"metric_for_best_model\": \"loss\",\n",
      "  \"greater_is_better\": false,\n",
      "  \"ignore_data_skip\": false,\n",
      "  \"fsdp\": [],\n",
      "  \"fsdp_min_num_params\": 0,\n",
      "  \"fsdp_config\": {\n",
      "    \"min_num_params\": 0,\n",
      "    \"xla\": false,\n",
      "    \"xla_fsdp_grad_ckpt\": false\n",
      "  },\n",
      "  \"fsdp_transformer_layer_cls_to_wrap\": null,\n",
      "  \"deepspeed\": null,\n",
      "  \"label_smoothing_factor\": 0.0,\n",
      "  \"optim\": \"paged_adamw_8bit\",\n",
      "  \"optim_args\": null,\n",
      "  \"adafactor\": false,\n",
      "  \"group_by_length\": false,\n",
      "  \"length_column_name\": \"length\",\n",
      "  \"report_to\": [\n",
      "    \"tensorboard\"\n",
      "  ],\n",
      "  \"ddp_find_unused_parameters\": null,\n",
      "  \"ddp_bucket_cap_mb\": null,\n",
      "  \"ddp_broadcast_buffers\": null,\n",
      "  \"dataloader_pin_memory\": true,\n",
      "  \"skip_memory_metrics\": true,\n",
      "  \"use_legacy_prediction_loop\": false,\n",
      "  \"push_to_hub\": false,\n",
      "  \"resume_from_checkpoint\": null,\n",
      "  \"hub_model_id\": null,\n",
      "  \"hub_strategy\": \"checkpoint\",\n",
      "  \"hub_token\": \"<HUB_TOKEN>\",\n",
      "  \"hub_private_repo\": true,\n",
      "  \"hub_always_push\": false,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"gradient_checkpointing_kwargs\": null,\n",
      "  \"include_inputs_for_metrics\": false,\n",
      "  \"fp16_backend\": \"auto\",\n",
      "  \"push_to_hub_model_id\": null,\n",
      "  \"push_to_hub_organization\": null,\n",
      "  \"push_to_hub_token\": \"<PUSH_TO_HUB_TOKEN>\",\n",
      "  \"mp_parameters\": \"\",\n",
      "  \"auto_find_batch_size\": false,\n",
      "  \"full_determinism\": false,\n",
      "  \"torchdynamo\": null,\n",
      "  \"ray_scope\": \"last\",\n",
      "  \"ddp_timeout\": 1800,\n",
      "  \"torch_compile\": false,\n",
      "  \"torch_compile_backend\": null,\n",
      "  \"torch_compile_mode\": null,\n",
      "  \"dispatch_batches\": null,\n",
      "  \"split_batches\": false,\n",
      "  \"include_tokens_per_second\": false,\n",
      "  \"neftune_noise_alpha\": null\n",
      "}\u001b[0m\n",
      "\u001b[32m2023-11-14 15:59:06.725\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mEval dataset is None\u001b[0m\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "Model config OPTConfig {\n",
      "  \"_name_or_path\": \"facebook/opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.35.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/vocab.json\n",
      "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "Model config OPTConfig {\n",
      "  \"_name_or_path\": \"facebook/opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.35.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "Model config OPTConfig {\n",
      "  \"_name_or_path\": \"facebook/opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.35.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "\u001b[32m2023-11-14 15:59:07.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mTokenizer facebook/opt-350m was built\u001b[0m\n",
      "\u001b[32m2023-11-14 15:59:07.025\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mCollator LMCollator was built\u001b[0m\n",
      "\u001b[32m2023-11-14 15:59:07.028\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mQuantization config is None. Model will be loaded using torch.float16\u001b[0m\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "Model config OPTConfig {\n",
      "  \"_name_or_path\": \"facebook/opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.35.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/pytorch_model.bin\n",
      "Instantiating OPTForCausalLM model under default dtype torch.float16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing OPTForCausalLM.\n",
      "\n",
      "All the weights of OPTForCausalLM were initialized from the model checkpoint at facebook/opt-350m.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use OPTForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "\u001b[32m2023-11-14 15:59:23.485\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mModel facebook/opt-350m was built\u001b[0m\n",
      "\u001b[32m2023-11-14 15:59:23.618\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mLoRA applied to the model facebook/opt-350m\u001b[0m\n",
      "\u001b[32m2023-11-14 15:59:23.628\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mModel facebook/opt-350m is stabilized for training\u001b[0m\n",
      "Using auto half precision backend\n",
      "\u001b[32m2023-11-14 15:59:23.882\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mTrainer LMTrainer was built\u001b[0m\n",
      "\u001b[32m2023-11-14 15:59:23.885\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mExperiment built successfully\u001b[0m\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 🚄 Run experiment"
   ],
   "metadata": {
    "id": "1WFN6ISmJnFm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "experiment.run()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 881
    },
    "id": "rzPdKTYW6_yF",
    "outputId": "8ce3719f-0907-4517-8717-2dc4d156af38"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[32m2023-11-14 15:59:23.895\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mTraining will start soon\u001b[0m\n",
      "***** Running training *****\n",
      "  Num examples = 300\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 150\n",
      "  Number of trainable parameters = 3,563,520\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 00:48, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.626900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.827700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.748600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.797700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.303100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.994300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.499000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.567500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.788300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.186500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.937400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.693600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.632500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.561100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.453600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.473700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Saving model checkpoint to ./outputs/checkpoint-100\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m2023-11-14 16:00:12.883\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mTraining end\u001b[0m\n",
      "\u001b[32m2023-11-14 16:00:12.888\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mModel saved to ./outputs/\u001b[0m\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 🎉 Done!\n",
    "\n",
    "You are trained a model using `xllm`"
   ],
   "metadata": {
    "id": "gU3Dy3FlKoCg"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fuse model"
   ],
   "metadata": {
    "id": "UPo9UQuptnuu"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# experiment.fuse_lora()"
   ],
   "metadata": {
    "id": "r__1558mtngt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get the model"
   ],
   "metadata": {
    "id": "BvihxRPHKuGd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# experiment.model"
   ],
   "metadata": {
    "id": "63ibIRNQLiTS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## You can save the model\n",
    "\n",
    "If you have not fuse the model, then only the LoRA weights will be saved."
   ],
   "metadata": {
    "id": "sfzXFLW2Lnpd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# experiment.model.save_pretrained(\"./trained_model/\")"
   ],
   "metadata": {
    "id": "wj6oheYCLkv5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## You could push the model to the HuggingFace Hub\n",
    "\n",
    "If you have not fuse the model, then only the LoRA weights will be saved.\n",
    "\n",
    "Make sure you are logged in HuggingFace Hub. You can run this command:\n",
    "\n",
    "```python\n",
    "!huggingface-cli login\n",
    "```\n",
    "\n",
    "Or you can set the environment variable with your Access token. You can find your token here: https://huggingface.co/settings/tokens\n",
    "\n",
    "```\n",
    "import os\n",
    "\n",
    "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"YOUR_ACCESS_TOKEN\"\n",
    "```"
   ],
   "metadata": {
    "id": "jgkDeErVLq-H"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# push the model and the tokenizer to the HuggingFace Hub\n",
    "# experiment.push_to_hub(\n",
    "#     repo_id=\"YOUR_LOGIN_AT_HF_HUB/MODEL_NAME\",\n",
    "#     private=False,\n",
    "#     safe_serialization=True\n",
    "# )"
   ],
   "metadata": {
    "id": "_jhP6AG_MIWJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 🎉 Done!\n",
    "\n",
    "You've trained the model using `xllm` and uploaded it to the hub"
   ],
   "metadata": {
    "id": "0nBPNvljMhQx"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Add QLoRA\n",
    "\n",
    "To train the `QLoRA` model, we need to load the backbone model using `bitsandbytes` library and int4 (or int8) weights."
   ],
   "metadata": {
    "id": "i0cnsI3Quizi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# config with QLoRA\n",
    "config = Config(\n",
    "    model_name_or_path=\"facebook/opt-350m\",\n",
    "    stabilize=True,\n",
    "    apply_lora=True,\n",
    "    load_in_4bit=True,\n",
    "    prepare_model_for_kbit_training=True,\n",
    ")"
   ],
   "metadata": {
    "id": "Jwj6AQjWvCPA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### You can explicitly specify the values of additional parameters in bitsandbytes quantization"
   ],
   "metadata": {
    "id": "MKya31NSvMHu"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# # extended config with QLoRA\n",
    "# config = Config(\n",
    "#     model_name_or_path=\"facebook/opt-350m\",\n",
    "#     stabilize=True,\n",
    "#     apply_lora=True,\n",
    "#     load_in_4bit=True,\n",
    "#     prepare_model_for_kbit_training=True,\n",
    "#     llm_int8_threshold=6.0,\n",
    "#     llm_int8_has_fp16_weight=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "# )"
   ],
   "metadata": {
    "id": "5Qq9t9VfvPg9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## All other steps are the same"
   ],
   "metadata": {
    "id": "jY7omBtEv2V6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_data = [\"Hello!\", \"How are you?\", \"Are you okay?\"] * 100\n",
    "train_dataset = GeneralDataset.from_list(data=train_data)\n",
    "experiment = Experiment(config=config, train_dataset=train_dataset)\n",
    "experiment.build()\n",
    "experiment.run()\n",
    "# experiment.fuse_lora()"
   ],
   "metadata": {
    "id": "FWhYuHl8v1xr",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "1f69aec1-d76e-4756-e9af-74d74bac532e"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[32m2023-11-14 16:00:12.955\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mExperiment building has started\u001b[0m\n",
      "\u001b[32m2023-11-14 16:00:12.957\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mConfig:\n",
      "{\n",
      "  \"experiment_key\": \"base\",\n",
      "  \"save_safetensors\": true,\n",
      "  \"max_shard_size\": \"10GB\",\n",
      "  \"local_rank\": 0,\n",
      "  \"use_gradient_checkpointing\": false,\n",
      "  \"trainer_key\": \"lm\",\n",
      "  \"force_fp32\": false,\n",
      "  \"force_fp16\": false,\n",
      "  \"from_gptq\": false,\n",
      "  \"huggingface_hub_token\": null,\n",
      "  \"deepspeed_stage\": 0,\n",
      "  \"deepspeed_config_path\": null,\n",
      "  \"fsdp_strategy\": \"\",\n",
      "  \"fsdp_offload\": true,\n",
      "  \"seed\": 42,\n",
      "  \"stabilize\": true,\n",
      "  \"path_to_env_file\": \"./.env\",\n",
      "  \"prepare_dataset\": true,\n",
      "  \"lora_hub_model_id\": null,\n",
      "  \"lora_model_local_path\": null,\n",
      "  \"fused_model_local_path\": null,\n",
      "  \"fuse_after_training\": false,\n",
      "  \"quantization_dataset_id\": null,\n",
      "  \"quantization_max_samples\": 1024,\n",
      "  \"quantized_model_path\": \"./quantized_model/\",\n",
      "  \"quantized_hub_model_id\": null,\n",
      "  \"quantized_hub_private_repo\": true,\n",
      "  \"dataset_key\": \"soda\",\n",
      "  \"train_local_path_to_data\": \"./train.jsonl\",\n",
      "  \"eval_local_path_to_data\": null,\n",
      "  \"shuffle\": true,\n",
      "  \"max_eval_samples\": 1000,\n",
      "  \"add_eval_to_train_if_no_path\": false,\n",
      "  \"tokenizer_name_or_path\": null,\n",
      "  \"tokenizer_use_fast\": null,\n",
      "  \"tokenizer_padding_side\": null,\n",
      "  \"collator_key\": \"lm\",\n",
      "  \"max_length\": 2048,\n",
      "  \"model_name_or_path\": \"facebook/opt-350m\",\n",
      "  \"push_to_hub_bos_add_bos_token\": false,\n",
      "  \"use_flash_attention_2\": false,\n",
      "  \"trust_remote_code\": false,\n",
      "  \"device_map\": null,\n",
      "  \"prepare_model_for_kbit_training\": true,\n",
      "  \"load_in_8bit\": false,\n",
      "  \"load_in_4bit\": true,\n",
      "  \"llm_int8_threshold\": 6.0,\n",
      "  \"llm_int8_has_fp16_weight\": true,\n",
      "  \"bnb_4bit_use_double_quant\": true,\n",
      "  \"bnb_4bit_quant_type\": \"nf4\",\n",
      "  \"bnb_quantize_after_model_init\": false,\n",
      "  \"gptq_bits\": 4,\n",
      "  \"gptq_group_size\": 128,\n",
      "  \"gptq_disable_exllama\": true,\n",
      "  \"apply_lora\": true,\n",
      "  \"lora_rank\": 8,\n",
      "  \"lora_alpha\": 32,\n",
      "  \"lora_dropout\": 0.1,\n",
      "  \"raw_lora_target_modules\": \"all\",\n",
      "  \"output_dir\": \"./outputs/\",\n",
      "  \"per_device_train_batch_size\": 2,\n",
      "  \"do_eval\": false,\n",
      "  \"per_device_eval_batch_size\": null,\n",
      "  \"gradient_accumulation_steps\": 1,\n",
      "  \"eval_accumulation_steps\": null,\n",
      "  \"eval_delay\": 0,\n",
      "  \"eval_steps\": 1000,\n",
      "  \"warmup_steps\": 1000,\n",
      "  \"max_steps\": null,\n",
      "  \"num_train_epochs\": 1,\n",
      "  \"learning_rate\": 0.0002,\n",
      "  \"max_grad_norm\": 1.0,\n",
      "  \"weight_decay\": 0.001,\n",
      "  \"label_smoothing_factor\": 0.0,\n",
      "  \"logging_steps\": 10,\n",
      "  \"save_steps\": 100,\n",
      "  \"save_total_limit\": 1,\n",
      "  \"optim\": \"paged_adamw_8bit\",\n",
      "  \"push_to_hub\": false,\n",
      "  \"hub_model_id\": null,\n",
      "  \"hub_private_repo\": true,\n",
      "  \"report_to_wandb\": false,\n",
      "  \"wandb_api_key\": null,\n",
      "  \"wandb_project\": null,\n",
      "  \"wandb_entity\": null\n",
      "}\u001b[0m\n",
      "\u001b[32m2023-11-14 16:00:12.960\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mConfig saved\u001b[0m\n",
      "\u001b[32m2023-11-14 16:00:12.962\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mChecks passed successfully\u001b[0m\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "\u001b[32m2023-11-14 16:00:12.973\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mTraining arguments was built:\n",
      "{\n",
      "  \"output_dir\": \"./outputs/\",\n",
      "  \"overwrite_output_dir\": false,\n",
      "  \"do_train\": false,\n",
      "  \"do_eval\": false,\n",
      "  \"do_predict\": false,\n",
      "  \"evaluation_strategy\": \"no\",\n",
      "  \"prediction_loss_only\": false,\n",
      "  \"per_device_train_batch_size\": 2,\n",
      "  \"per_device_eval_batch_size\": 2,\n",
      "  \"per_gpu_train_batch_size\": null,\n",
      "  \"per_gpu_eval_batch_size\": null,\n",
      "  \"gradient_accumulation_steps\": 1,\n",
      "  \"eval_accumulation_steps\": 1,\n",
      "  \"eval_delay\": 0,\n",
      "  \"learning_rate\": 0.0002,\n",
      "  \"weight_decay\": 0.001,\n",
      "  \"adam_beta1\": 0.9,\n",
      "  \"adam_beta2\": 0.999,\n",
      "  \"adam_epsilon\": 1e-08,\n",
      "  \"max_grad_norm\": 1.0,\n",
      "  \"num_train_epochs\": 1,\n",
      "  \"max_steps\": -1,\n",
      "  \"lr_scheduler_type\": \"linear\",\n",
      "  \"warmup_ratio\": 0.0,\n",
      "  \"warmup_steps\": 1000,\n",
      "  \"log_level\": \"info\",\n",
      "  \"log_level_replica\": \"warning\",\n",
      "  \"log_on_each_node\": true,\n",
      "  \"logging_dir\": \"./outputs/runs/Nov14_16-00-12_735f762378cc\",\n",
      "  \"logging_strategy\": \"steps\",\n",
      "  \"logging_first_step\": true,\n",
      "  \"logging_steps\": 10,\n",
      "  \"logging_nan_inf_filter\": true,\n",
      "  \"save_strategy\": \"steps\",\n",
      "  \"save_steps\": 100,\n",
      "  \"save_total_limit\": 1,\n",
      "  \"save_safetensors\": true,\n",
      "  \"save_on_each_node\": false,\n",
      "  \"no_cuda\": false,\n",
      "  \"use_cpu\": false,\n",
      "  \"use_mps_device\": false,\n",
      "  \"seed\": 42,\n",
      "  \"data_seed\": 42,\n",
      "  \"jit_mode_eval\": false,\n",
      "  \"use_ipex\": false,\n",
      "  \"bf16\": false,\n",
      "  \"fp16\": true,\n",
      "  \"fp16_opt_level\": \"O1\",\n",
      "  \"half_precision_backend\": \"auto\",\n",
      "  \"bf16_full_eval\": false,\n",
      "  \"fp16_full_eval\": false,\n",
      "  \"tf32\": null,\n",
      "  \"local_rank\": 0,\n",
      "  \"ddp_backend\": null,\n",
      "  \"tpu_num_cores\": null,\n",
      "  \"tpu_metrics_debug\": false,\n",
      "  \"debug\": [],\n",
      "  \"dataloader_drop_last\": false,\n",
      "  \"eval_steps\": 1000,\n",
      "  \"dataloader_num_workers\": 0,\n",
      "  \"past_index\": -1,\n",
      "  \"run_name\": \"./outputs/\",\n",
      "  \"disable_tqdm\": false,\n",
      "  \"remove_unused_columns\": false,\n",
      "  \"label_names\": null,\n",
      "  \"load_best_model_at_end\": false,\n",
      "  \"metric_for_best_model\": \"loss\",\n",
      "  \"greater_is_better\": false,\n",
      "  \"ignore_data_skip\": false,\n",
      "  \"fsdp\": [],\n",
      "  \"fsdp_min_num_params\": 0,\n",
      "  \"fsdp_config\": {\n",
      "    \"min_num_params\": 0,\n",
      "    \"xla\": false,\n",
      "    \"xla_fsdp_grad_ckpt\": false\n",
      "  },\n",
      "  \"fsdp_transformer_layer_cls_to_wrap\": null,\n",
      "  \"deepspeed\": null,\n",
      "  \"label_smoothing_factor\": 0.0,\n",
      "  \"optim\": \"paged_adamw_8bit\",\n",
      "  \"optim_args\": null,\n",
      "  \"adafactor\": false,\n",
      "  \"group_by_length\": false,\n",
      "  \"length_column_name\": \"length\",\n",
      "  \"report_to\": [\n",
      "    \"tensorboard\"\n",
      "  ],\n",
      "  \"ddp_find_unused_parameters\": null,\n",
      "  \"ddp_bucket_cap_mb\": null,\n",
      "  \"ddp_broadcast_buffers\": null,\n",
      "  \"dataloader_pin_memory\": true,\n",
      "  \"skip_memory_metrics\": true,\n",
      "  \"use_legacy_prediction_loop\": false,\n",
      "  \"push_to_hub\": false,\n",
      "  \"resume_from_checkpoint\": null,\n",
      "  \"hub_model_id\": null,\n",
      "  \"hub_strategy\": \"checkpoint\",\n",
      "  \"hub_token\": \"<HUB_TOKEN>\",\n",
      "  \"hub_private_repo\": true,\n",
      "  \"hub_always_push\": false,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"gradient_checkpointing_kwargs\": null,\n",
      "  \"include_inputs_for_metrics\": false,\n",
      "  \"fp16_backend\": \"auto\",\n",
      "  \"push_to_hub_model_id\": null,\n",
      "  \"push_to_hub_organization\": null,\n",
      "  \"push_to_hub_token\": \"<PUSH_TO_HUB_TOKEN>\",\n",
      "  \"mp_parameters\": \"\",\n",
      "  \"auto_find_batch_size\": false,\n",
      "  \"full_determinism\": false,\n",
      "  \"torchdynamo\": null,\n",
      "  \"ray_scope\": \"last\",\n",
      "  \"ddp_timeout\": 1800,\n",
      "  \"torch_compile\": false,\n",
      "  \"torch_compile_backend\": null,\n",
      "  \"torch_compile_mode\": null,\n",
      "  \"dispatch_batches\": null,\n",
      "  \"split_batches\": false,\n",
      "  \"include_tokens_per_second\": false,\n",
      "  \"neftune_noise_alpha\": null\n",
      "}\u001b[0m\n",
      "\u001b[32m2023-11-14 16:00:12.977\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mEval dataset is None\u001b[0m\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "Model config OPTConfig {\n",
      "  \"_name_or_path\": \"facebook/opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.35.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/vocab.json\n",
      "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "Model config OPTConfig {\n",
      "  \"_name_or_path\": \"facebook/opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.35.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "Model config OPTConfig {\n",
      "  \"_name_or_path\": \"facebook/opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.35.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "\u001b[32m2023-11-14 16:00:13.378\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mTokenizer facebook/opt-350m was built\u001b[0m\n",
      "\u001b[32m2023-11-14 16:00:13.380\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mCollator LMCollator was built\u001b[0m\n",
      "\u001b[32m2023-11-14 16:00:13.386\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mQuantization config was built:\n",
      "{\n",
      "  \"bnb_4bit_compute_dtype\": \"float16\",\n",
      "  \"bnb_4bit_quant_type\": \"nf4\",\n",
      "  \"bnb_4bit_use_double_quant\": true,\n",
      "  \"llm_int8_has_fp16_weight\": true,\n",
      "  \"load_in_4bit\": true\n",
      "}\n",
      "\u001b[0m\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "Model config OPTConfig {\n",
      "  \"_name_or_path\": \"facebook/opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.35.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "The device_map was not initialized. Setting device_map to {'':torch.cuda.current_device()}. If you want to use the model for inference, please set device_map ='auto' \n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/pytorch_model.bin\n",
      "Instantiating OPTForCausalLM model under default dtype torch.float16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Detected 4-bit loading: activating 4-bit loading for this model\n",
      "All model checkpoint weights were used when initializing OPTForCausalLM.\n",
      "\n",
      "All the weights of OPTForCausalLM were initialized from the model checkpoint at facebook/opt-350m.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use OPTForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "\u001b[32m2023-11-14 16:00:15.797\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mModel prepared for kbit training. Gradient checkpointing: False\u001b[0m\n",
      "\u001b[32m2023-11-14 16:00:15.799\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mModel facebook/opt-350m was built\u001b[0m\n",
      "\u001b[32m2023-11-14 16:00:15.957\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mLoRA applied to the model facebook/opt-350m\u001b[0m\n",
      "\u001b[32m2023-11-14 16:00:15.966\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mModel facebook/opt-350m is stabilized for training\u001b[0m\n",
      "Using auto half precision backend\n",
      "\u001b[32m2023-11-14 16:00:15.976\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mTrainer LMTrainer was built\u001b[0m\n",
      "\u001b[32m2023-11-14 16:00:15.977\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mExperiment built successfully\u001b[0m\n",
      "\u001b[32m2023-11-14 16:00:15.981\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mTraining will start soon\u001b[0m\n",
      "***** Running training *****\n",
      "  Num examples = 300\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 150\n",
      "  Number of trainable parameters = 3,563,520\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 00:53, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.945300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.768000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.720400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>4.408000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.776000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.216700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.629700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.522100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.713300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.168800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.829100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.735500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.586600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.422200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.428500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Saving model checkpoint to ./outputs/checkpoint-100\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m2023-11-14 16:01:09.884\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mTraining end\u001b[0m\n",
      "\u001b[32m2023-11-14 16:01:09.888\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mModel saved to ./outputs/\u001b[0m\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## You also can add `Gradient Checkpointing`\n",
    "\n",
    "This will help to use `less GPU memory` during training, that is, you will be able to learn more than without this technique. The disadvantages of this technique is slowing down the forward step, that is, `slowing down training`.\n",
    "\n",
    "Summarizing: you will be training larger models (for example 7B in colab), but at the expense of training speed."
   ],
   "metadata": {
    "id": "rYpCTXD1z48a"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# config = Config(\n",
    "#     model_name_or_path=\"facebook/opt-350m\",\n",
    "\n",
    "#     use_gradient_checkpointing=True,\n",
    "\n",
    "#     stabilize=True,\n",
    "#     apply_lora=True,\n",
    "#     load_in_4bit=True,\n",
    "#     prepare_model_for_kbit_training=True,\n",
    "# )"
   ],
   "metadata": {
    "id": "4-_uG-aN0s8F"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Add eval data"
   ],
   "metadata": {
    "id": "UOG_A3itJzvR"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup config\n",
    "\n",
    "- `do_eval` for turn on evaluation  \n",
    "- `eval_steps` how often we should run evaluation"
   ],
   "metadata": {
    "id": "Kw7FFI0TJ6hc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "config = Config(\n",
    "    model_name_or_path=\"facebook/opt-350m\",\n",
    "    stabilize=True,\n",
    "    apply_lora=True,\n",
    "    load_in_4bit=True,\n",
    "    prepare_model_for_kbit_training=True,\n",
    "    do_eval=True,\n",
    "    eval_steps=50,\n",
    ")"
   ],
   "metadata": {
    "id": "ue9wpOUy6--4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Make dummy eval dataset"
   ],
   "metadata": {
    "id": "xMIA8MwoKM8V"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "eval_data = [\"Hi\", \"Sup?\"] * 10"
   ],
   "metadata": {
    "id": "BLEMaWo2HJVs"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Make a `xllm` eval dataset"
   ],
   "metadata": {
    "id": "EF2U5TZMKUDW"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "eval_dataset = GeneralDataset.from_list(eval_data)"
   ],
   "metadata": {
    "id": "exF7XB2AKRcn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Init experiment with the `eval_dataset`"
   ],
   "metadata": {
    "id": "oyHOGsJuQg0D"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "experiment = Experiment(config=config, train_dataset=train_dataset, eval_dataset=eval_dataset)"
   ],
   "metadata": {
    "id": "ps856ZWwKdYg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build experiment"
   ],
   "metadata": {
    "id": "DC81sYGfQkDy"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "experiment.build()"
   ],
   "metadata": {
    "id": "k5LKSvT3KaYS",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "29a263e6-28a4-4774-90bb-f3752af59ce5"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[32m2023-11-14 16:01:09.943\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mExperiment building has started\u001b[0m\n",
      "\u001b[32m2023-11-14 16:01:09.947\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mConfig:\n",
      "{\n",
      "  \"experiment_key\": \"base\",\n",
      "  \"save_safetensors\": true,\n",
      "  \"max_shard_size\": \"10GB\",\n",
      "  \"local_rank\": 0,\n",
      "  \"use_gradient_checkpointing\": false,\n",
      "  \"trainer_key\": \"lm\",\n",
      "  \"force_fp32\": false,\n",
      "  \"force_fp16\": false,\n",
      "  \"from_gptq\": false,\n",
      "  \"huggingface_hub_token\": null,\n",
      "  \"deepspeed_stage\": 0,\n",
      "  \"deepspeed_config_path\": null,\n",
      "  \"fsdp_strategy\": \"\",\n",
      "  \"fsdp_offload\": true,\n",
      "  \"seed\": 42,\n",
      "  \"stabilize\": true,\n",
      "  \"path_to_env_file\": \"./.env\",\n",
      "  \"prepare_dataset\": true,\n",
      "  \"lora_hub_model_id\": null,\n",
      "  \"lora_model_local_path\": null,\n",
      "  \"fused_model_local_path\": null,\n",
      "  \"fuse_after_training\": false,\n",
      "  \"quantization_dataset_id\": null,\n",
      "  \"quantization_max_samples\": 1024,\n",
      "  \"quantized_model_path\": \"./quantized_model/\",\n",
      "  \"quantized_hub_model_id\": null,\n",
      "  \"quantized_hub_private_repo\": true,\n",
      "  \"dataset_key\": \"soda\",\n",
      "  \"train_local_path_to_data\": \"./train.jsonl\",\n",
      "  \"eval_local_path_to_data\": null,\n",
      "  \"shuffle\": true,\n",
      "  \"max_eval_samples\": 1000,\n",
      "  \"add_eval_to_train_if_no_path\": false,\n",
      "  \"tokenizer_name_or_path\": null,\n",
      "  \"tokenizer_use_fast\": null,\n",
      "  \"tokenizer_padding_side\": null,\n",
      "  \"collator_key\": \"lm\",\n",
      "  \"max_length\": 2048,\n",
      "  \"model_name_or_path\": \"facebook/opt-350m\",\n",
      "  \"push_to_hub_bos_add_bos_token\": false,\n",
      "  \"use_flash_attention_2\": false,\n",
      "  \"trust_remote_code\": false,\n",
      "  \"device_map\": null,\n",
      "  \"prepare_model_for_kbit_training\": true,\n",
      "  \"load_in_8bit\": false,\n",
      "  \"load_in_4bit\": true,\n",
      "  \"llm_int8_threshold\": 6.0,\n",
      "  \"llm_int8_has_fp16_weight\": true,\n",
      "  \"bnb_4bit_use_double_quant\": true,\n",
      "  \"bnb_4bit_quant_type\": \"nf4\",\n",
      "  \"bnb_quantize_after_model_init\": false,\n",
      "  \"gptq_bits\": 4,\n",
      "  \"gptq_group_size\": 128,\n",
      "  \"gptq_disable_exllama\": true,\n",
      "  \"apply_lora\": true,\n",
      "  \"lora_rank\": 8,\n",
      "  \"lora_alpha\": 32,\n",
      "  \"lora_dropout\": 0.1,\n",
      "  \"raw_lora_target_modules\": \"all\",\n",
      "  \"output_dir\": \"./outputs/\",\n",
      "  \"per_device_train_batch_size\": 2,\n",
      "  \"do_eval\": true,\n",
      "  \"per_device_eval_batch_size\": null,\n",
      "  \"gradient_accumulation_steps\": 1,\n",
      "  \"eval_accumulation_steps\": null,\n",
      "  \"eval_delay\": 0,\n",
      "  \"eval_steps\": 50,\n",
      "  \"warmup_steps\": 1000,\n",
      "  \"max_steps\": null,\n",
      "  \"num_train_epochs\": 1,\n",
      "  \"learning_rate\": 0.0002,\n",
      "  \"max_grad_norm\": 1.0,\n",
      "  \"weight_decay\": 0.001,\n",
      "  \"label_smoothing_factor\": 0.0,\n",
      "  \"logging_steps\": 10,\n",
      "  \"save_steps\": 100,\n",
      "  \"save_total_limit\": 1,\n",
      "  \"optim\": \"paged_adamw_8bit\",\n",
      "  \"push_to_hub\": false,\n",
      "  \"hub_model_id\": null,\n",
      "  \"hub_private_repo\": true,\n",
      "  \"report_to_wandb\": false,\n",
      "  \"wandb_api_key\": null,\n",
      "  \"wandb_project\": null,\n",
      "  \"wandb_entity\": null\n",
      "}\u001b[0m\n",
      "\u001b[32m2023-11-14 16:01:09.949\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mConfig saved\u001b[0m\n",
      "\u001b[32m2023-11-14 16:01:09.950\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mChecks passed successfully\u001b[0m\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "\u001b[32m2023-11-14 16:01:09.957\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mTraining arguments was built:\n",
      "{\n",
      "  \"output_dir\": \"./outputs/\",\n",
      "  \"overwrite_output_dir\": false,\n",
      "  \"do_train\": false,\n",
      "  \"do_eval\": true,\n",
      "  \"do_predict\": false,\n",
      "  \"evaluation_strategy\": \"steps\",\n",
      "  \"prediction_loss_only\": false,\n",
      "  \"per_device_train_batch_size\": 2,\n",
      "  \"per_device_eval_batch_size\": 2,\n",
      "  \"per_gpu_train_batch_size\": null,\n",
      "  \"per_gpu_eval_batch_size\": null,\n",
      "  \"gradient_accumulation_steps\": 1,\n",
      "  \"eval_accumulation_steps\": 1,\n",
      "  \"eval_delay\": 0,\n",
      "  \"learning_rate\": 0.0002,\n",
      "  \"weight_decay\": 0.001,\n",
      "  \"adam_beta1\": 0.9,\n",
      "  \"adam_beta2\": 0.999,\n",
      "  \"adam_epsilon\": 1e-08,\n",
      "  \"max_grad_norm\": 1.0,\n",
      "  \"num_train_epochs\": 1,\n",
      "  \"max_steps\": -1,\n",
      "  \"lr_scheduler_type\": \"linear\",\n",
      "  \"warmup_ratio\": 0.0,\n",
      "  \"warmup_steps\": 1000,\n",
      "  \"log_level\": \"info\",\n",
      "  \"log_level_replica\": \"warning\",\n",
      "  \"log_on_each_node\": true,\n",
      "  \"logging_dir\": \"./outputs/runs/Nov14_16-01-09_735f762378cc\",\n",
      "  \"logging_strategy\": \"steps\",\n",
      "  \"logging_first_step\": true,\n",
      "  \"logging_steps\": 10,\n",
      "  \"logging_nan_inf_filter\": true,\n",
      "  \"save_strategy\": \"steps\",\n",
      "  \"save_steps\": 100,\n",
      "  \"save_total_limit\": 1,\n",
      "  \"save_safetensors\": true,\n",
      "  \"save_on_each_node\": false,\n",
      "  \"no_cuda\": false,\n",
      "  \"use_cpu\": false,\n",
      "  \"use_mps_device\": false,\n",
      "  \"seed\": 42,\n",
      "  \"data_seed\": 42,\n",
      "  \"jit_mode_eval\": false,\n",
      "  \"use_ipex\": false,\n",
      "  \"bf16\": false,\n",
      "  \"fp16\": true,\n",
      "  \"fp16_opt_level\": \"O1\",\n",
      "  \"half_precision_backend\": \"auto\",\n",
      "  \"bf16_full_eval\": false,\n",
      "  \"fp16_full_eval\": false,\n",
      "  \"tf32\": null,\n",
      "  \"local_rank\": 0,\n",
      "  \"ddp_backend\": null,\n",
      "  \"tpu_num_cores\": null,\n",
      "  \"tpu_metrics_debug\": false,\n",
      "  \"debug\": [],\n",
      "  \"dataloader_drop_last\": false,\n",
      "  \"eval_steps\": 50,\n",
      "  \"dataloader_num_workers\": 0,\n",
      "  \"past_index\": -1,\n",
      "  \"run_name\": \"./outputs/\",\n",
      "  \"disable_tqdm\": false,\n",
      "  \"remove_unused_columns\": false,\n",
      "  \"label_names\": null,\n",
      "  \"load_best_model_at_end\": false,\n",
      "  \"metric_for_best_model\": \"eval_loss\",\n",
      "  \"greater_is_better\": false,\n",
      "  \"ignore_data_skip\": false,\n",
      "  \"fsdp\": [],\n",
      "  \"fsdp_min_num_params\": 0,\n",
      "  \"fsdp_config\": {\n",
      "    \"min_num_params\": 0,\n",
      "    \"xla\": false,\n",
      "    \"xla_fsdp_grad_ckpt\": false\n",
      "  },\n",
      "  \"fsdp_transformer_layer_cls_to_wrap\": null,\n",
      "  \"deepspeed\": null,\n",
      "  \"label_smoothing_factor\": 0.0,\n",
      "  \"optim\": \"paged_adamw_8bit\",\n",
      "  \"optim_args\": null,\n",
      "  \"adafactor\": false,\n",
      "  \"group_by_length\": false,\n",
      "  \"length_column_name\": \"length\",\n",
      "  \"report_to\": [\n",
      "    \"tensorboard\"\n",
      "  ],\n",
      "  \"ddp_find_unused_parameters\": null,\n",
      "  \"ddp_bucket_cap_mb\": null,\n",
      "  \"ddp_broadcast_buffers\": null,\n",
      "  \"dataloader_pin_memory\": true,\n",
      "  \"skip_memory_metrics\": true,\n",
      "  \"use_legacy_prediction_loop\": false,\n",
      "  \"push_to_hub\": false,\n",
      "  \"resume_from_checkpoint\": null,\n",
      "  \"hub_model_id\": null,\n",
      "  \"hub_strategy\": \"checkpoint\",\n",
      "  \"hub_token\": \"<HUB_TOKEN>\",\n",
      "  \"hub_private_repo\": true,\n",
      "  \"hub_always_push\": false,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"gradient_checkpointing_kwargs\": null,\n",
      "  \"include_inputs_for_metrics\": false,\n",
      "  \"fp16_backend\": \"auto\",\n",
      "  \"push_to_hub_model_id\": null,\n",
      "  \"push_to_hub_organization\": null,\n",
      "  \"push_to_hub_token\": \"<PUSH_TO_HUB_TOKEN>\",\n",
      "  \"mp_parameters\": \"\",\n",
      "  \"auto_find_batch_size\": false,\n",
      "  \"full_determinism\": false,\n",
      "  \"torchdynamo\": null,\n",
      "  \"ray_scope\": \"last\",\n",
      "  \"ddp_timeout\": 1800,\n",
      "  \"torch_compile\": false,\n",
      "  \"torch_compile_backend\": null,\n",
      "  \"torch_compile_mode\": null,\n",
      "  \"dispatch_batches\": null,\n",
      "  \"split_batches\": false,\n",
      "  \"include_tokens_per_second\": false,\n",
      "  \"neftune_noise_alpha\": null\n",
      "}\u001b[0m\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "Model config OPTConfig {\n",
      "  \"_name_or_path\": \"facebook/opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.35.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/vocab.json\n",
      "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "Model config OPTConfig {\n",
      "  \"_name_or_path\": \"facebook/opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.35.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "Model config OPTConfig {\n",
      "  \"_name_or_path\": \"facebook/opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.35.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "\u001b[32m2023-11-14 16:01:10.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mTokenizer facebook/opt-350m was built\u001b[0m\n",
      "\u001b[32m2023-11-14 16:01:10.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mCollator LMCollator was built\u001b[0m\n",
      "\u001b[32m2023-11-14 16:01:10.249\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mQuantization config was built:\n",
      "{\n",
      "  \"bnb_4bit_compute_dtype\": \"float16\",\n",
      "  \"bnb_4bit_quant_type\": \"nf4\",\n",
      "  \"bnb_4bit_use_double_quant\": true,\n",
      "  \"llm_int8_has_fp16_weight\": true,\n",
      "  \"load_in_4bit\": true\n",
      "}\n",
      "\u001b[0m\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "Model config OPTConfig {\n",
      "  \"_name_or_path\": \"facebook/opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.35.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "The device_map was not initialized. Setting device_map to {'':torch.cuda.current_device()}. If you want to use the model for inference, please set device_map ='auto' \n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/pytorch_model.bin\n",
      "Instantiating OPTForCausalLM model under default dtype torch.float16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Detected 4-bit loading: activating 4-bit loading for this model\n",
      "All model checkpoint weights were used when initializing OPTForCausalLM.\n",
      "\n",
      "All the weights of OPTForCausalLM were initialized from the model checkpoint at facebook/opt-350m.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use OPTForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "\u001b[32m2023-11-14 16:01:12.087\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mModel prepared for kbit training. Gradient checkpointing: False\u001b[0m\n",
      "\u001b[32m2023-11-14 16:01:12.089\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mModel facebook/opt-350m was built\u001b[0m\n",
      "\u001b[32m2023-11-14 16:01:12.249\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mLoRA applied to the model facebook/opt-350m\u001b[0m\n",
      "\u001b[32m2023-11-14 16:01:12.258\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mModel facebook/opt-350m is stabilized for training\u001b[0m\n",
      "Using auto half precision backend\n",
      "\u001b[32m2023-11-14 16:01:12.265\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mTrainer LMTrainer was built\u001b[0m\n",
      "\u001b[32m2023-11-14 16:01:12.267\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mExperiment built successfully\u001b[0m\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run experiment"
   ],
   "metadata": {
    "id": "jRCjV7UEQlhp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "experiment.run()"
   ],
   "metadata": {
    "id": "NadhNd39KRRq",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 634
    },
    "outputId": "93ad2638-92fe-4e40-9e45-1db33e0f82e5"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[32m2023-11-14 16:01:12.278\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mTraining will start soon\u001b[0m\n",
      "***** Running training *****\n",
      "  Num examples = 300\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 150\n",
      "  Number of trainable parameters = 3,563,520\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 00:56, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.216700</td>\n",
       "      <td>7.310188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.829100</td>\n",
       "      <td>8.282642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.428500</td>\n",
       "      <td>8.972834</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 2\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./outputs/checkpoint-100\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 2\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m2023-11-14 16:02:09.046\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mTraining end\u001b[0m\n",
      "\u001b[32m2023-11-14 16:02:09.048\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mModel saved to ./outputs/\u001b[0m\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 🎉 You are awesome!\n",
    "\n",
    "## Now you know how to prototype models using `xllm`\n",
    "\n",
    "### Explore more examples at X—LLM repo\n",
    "\n",
    "https://github.com/BobaZooba/xllm\n",
    "\n",
    "Useful materials:\n",
    "\n",
    "- [X—LLM Repo](https://github.com/BobaZooba/xllm): main repo of the `xllm` library\n",
    "- [Quickstart](https://github.com/KompleteAI/xllm/tree/docs-v1#quickstart-): basics of `xllm`\n",
    "- [Examples](https://github.com/BobaZooba/xllm/examples): minimal examples of using `xllm`\n",
    "- [Guide](https://github.com/BobaZooba/xllm/blob/main/GUIDE.md): here, we go into detail about everything the library can\n",
    "  do\n",
    "- [Demo project](https://github.com/BobaZooba/xllm-demo): here's a minimal step-by-step example of how to use X—LLM and fit it\n",
    "  into your own project\n",
    "- [WeatherGPT](https://github.com/BobaZooba/wgpt): this repository features an example of how to utilize the xllm library. Included is a solution for a common type of assessment given to LLM engineers, who typically earn between $120,000 to $140,000 annually\n",
    "- [Shurale](https://github.com/BobaZooba/shurale): project with the finetuned 7B Mistal model\n"
   ],
   "metadata": {
    "id": "NlX7tO65hOQU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tale Quest\n",
    "\n",
    "`Tale Quest` is my personal project which was built using `xllm` and `Shurale`. It's an interactive text-based game\n",
    "in `Telegram` with dynamic AI characters, offering infinite scenarios\n",
    "\n",
    "You will get into exciting journeys and complete fascinating quests. Chat\n",
    "with `George Orwell`, `Tech Entrepreneur`, `Young Wizard`, `Noir Detective`, `Femme Fatale` and many more\n",
    "\n",
    "Try it now: [https://t.me/talequestbot](https://t.me/TaleQuestBot?start=Z2g)"
   ],
   "metadata": {
    "id": "5wJJrKnglAkK"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "fFfDfFiCpDPv"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
