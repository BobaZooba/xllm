{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQQ_O9Wmsgxw"
   },
   "source": [
    "# Llama2 & Mistral AI efficient fine-tuning using QLoRA, bnb int4, gradient checkpointing and Xâ€”LLM ðŸ¦–\n",
    "\n",
    "- [Xâ€”LLM Repo](https://github.com/BobaZooba/xllm): main repo of the `xllm` library\n",
    "- [Quickstart](https://github.com/KompleteAI/xllm/tree/docs-v1#quickstart-): basics of `xllm`\n",
    "- [Examples](https://github.com/BobaZooba/xllm/examples): minimal examples of using `xllm`\n",
    "- [Guide](https://github.com/BobaZooba/xllm/blob/main/GUIDE.md): here, we go into detail about everything the library can\n",
    "  do\n",
    "- [Demo project](https://github.com/BobaZooba/xllm-demo): here's a minimal step-by-step example of how to use Xâ€”LLM and fit it\n",
    "  into your own project\n",
    "- [WeatherGPT](https://github.com/BobaZooba/wgpt): this repository features an example of how to utilize the xllm library. Included is a solution for a common type of assessment given to LLM engineers, who typically earn between $120,000 to $140,000 annually\n",
    "- [Shurale](https://github.com/BobaZooba/shurale): project with the finetuned 7B Mistal model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVEUs8X5rTiV"
   },
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c4jm8Qr2qMuU",
    "outputId": "82f4f3e2-b9bc-40ac-f70d-1d7fb9d35bce"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: xllm in /usr/local/lib/python3.10/dist-packages (0.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from xllm) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from xllm) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from xllm) (5.9.5)\n",
      "Requirement already satisfied: torch>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from xllm) (2.1.0+cu118)\n",
      "Requirement already satisfied: loguru in /usr/local/lib/python3.10/dist-packages (from xllm) (0.7.2)\n",
      "Requirement already satisfied: peft>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from xllm) (0.6.2)\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (from xllm) (0.16.0)\n",
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from xllm) (1.0.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from xllm) (2.31.0)\n",
      "Requirement already satisfied: optimum>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from xllm) (1.14.1)\n",
      "Requirement already satisfied: bitsandbytes>=0.41.1 in /usr/local/lib/python3.10/dist-packages (from xllm) (0.41.2.post2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xllm) (1.11.3)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from xllm) (4.35.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from xllm) (4.66.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from xllm) (0.4.0)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from optimum>=1.12.0->xllm) (15.0.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from optimum>=1.12.0->xllm) (1.12)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from optimum>=1.12.0->xllm) (0.17.3)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from optimum>=1.12.0->xllm) (2.14.7)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft>=0.5.0->xllm) (6.0.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft>=0.5.0->xllm) (0.24.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->xllm) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->xllm) (4.5.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->xllm) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->xllm) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->xllm) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->xllm) (2.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->xllm) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->xllm) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->xllm) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->xllm) (2023.7.22)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->xllm) (2023.6.3)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->xllm) (0.14.1)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb->xllm) (8.1.7)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->xllm) (3.1.40)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->xllm) (1.35.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb->xllm) (0.4.0)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb->xllm) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb->xllm) (67.7.2)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb->xllm) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb->xllm) (3.20.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb->xllm) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb->xllm) (4.0.11)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers->xllm) (0.1.99)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->optimum>=1.12.0->xllm) (10.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->optimum>=1.12.0->xllm) (9.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->optimum>=1.12.0->xllm) (0.5)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->optimum>=1.12.0->xllm) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->optimum>=1.12.0->xllm) (1.5.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->optimum>=1.12.0->xllm) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->optimum>=1.12.0->xllm) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->optimum>=1.12.0->xllm) (3.8.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.1->xllm) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->optimum>=1.12.0->xllm) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum>=1.12.0->xllm) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum>=1.12.0->xllm) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum>=1.12.0->xllm) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum>=1.12.0->xllm) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum>=1.12.0->xllm) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum>=1.12.0->xllm) (1.3.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->xllm) (5.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->optimum>=1.12.0->xllm) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->optimum>=1.12.0->xllm) (2023.3.post1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade xllm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_xO5kdESEUl"
   },
   "source": [
    "# Login to HuggingFace to save model to the hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "c_BRWCP9FHnG"
   },
   "outputs": [],
   "source": [
    "# !huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ksKCydbdy5Dp"
   },
   "source": [
    "# [Optional] Login to W&B to save training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8ZvYGsYXyuaY"
   },
   "outputs": [],
   "source": [
    "# !wandb login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tiine2-9rVpc"
   },
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BuBIZNOQqZOX",
    "outputId": "99e6deaa-53e3-4006-8a85-b7f91d60492b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Xâ€”LLM version: 0.1.0\n",
      "Torch version: 2.1.0+cu118\n",
      "Cuda is available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import xllm\n",
    "\n",
    "cuda_is_available = torch.cuda.is_available()\n",
    "\n",
    "print(f\"Xâ€”LLM version: {xllm.__version__}\\nTorch version: {torch.__version__}\\nCuda is available: {cuda_is_available}\")\n",
    "assert cuda_is_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "DbDhVyrrqbUe"
   },
   "outputs": [],
   "source": [
    "from xllm import Config\n",
    "from xllm.datasets import GeneralDataset\n",
    "from xllm.experiments import Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cU3jAEAgSVmh"
   },
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BF8PadIirHHJ"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Anthropic/hh-rlhf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "V5IzE3c8rIZf"
   },
   "outputs": [],
   "source": [
    "train_data = list()\n",
    "\n",
    "for sample in dataset[\"train\"]:\n",
    "    train_data.append({\"text\": sample[\"chosen\"].strip()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TdAM7XwMrmZy",
    "outputId": "5af6f7ed-1676-4285-ac11-d211990df700"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "160800"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oTOWzJcCro_5",
    "outputId": "6b734afc-1af5-473d-acb0-0cc3455af52d"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'text': 'Human: Can you provide me the home address for Senator Elizabeth Warren?\\n\\nAssistant: Sure, whatâ€™s your question?'}"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "train_data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Xj6dzaRC_1UD"
   },
   "outputs": [],
   "source": [
    "train_dataset = GeneralDataset(data=train_data, separator=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "apFeqwVH_1Rf",
    "outputId": "ac46909b-ba98-4b32-8b77-6262e3f76f9c"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'text_parts': ['Human: Can you provide me the home address for Senator Elizabeth Warren?',\n",
       "  'Assistant: Sure, whatâ€™s your question?']}"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "train_dataset[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjkDiGjqSX8-"
   },
   "source": [
    "# Make a Xâ€”LLM config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "-a3MAkBFSKD_"
   },
   "outputs": [],
   "source": [
    "config = Config(\n",
    "    collator_key=\"lm\",\n",
    "    use_gradient_checkpointing=True,\n",
    "    # model_name_or_path=\"TinyPixel/Llama-2-7B-bf16-sharded\",\n",
    "    model_name_or_path=\"bn22/Mistral-7B-v0.1-sharded\",\n",
    "    use_flash_attention_2=False,  # not supported in colab\n",
    "    load_in_4bit=True,\n",
    "    prepare_model_for_kbit_training=True,\n",
    "    apply_lora=True,\n",
    "    warmup_steps=5,\n",
    "    max_steps=25,\n",
    "    logging_steps=1,\n",
    "    save_steps=25,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    max_length=2048,\n",
    "    # tokenizer_padding_side=\"right\",  # good for llama2\n",
    "    # ATTENTON: set your values\n",
    "    push_to_hub=False,\n",
    "    hub_private_repo=True,\n",
    "    hub_model_id=\"BobaZooba/AntModel-7B-XLLM-Demo-LoRA\",\n",
    "    # W&B\n",
    "    report_to_wandb=False,\n",
    "    wandb_project=\"xllm-demo\",\n",
    "    wandb_entity=\"bobazooba\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jcXUNRykSdmG"
   },
   "source": [
    "# Make a Xâ€”LLM experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "gUiDbGwm_1I7"
   },
   "outputs": [],
   "source": [
    "experiment = Experiment(config=config, train_dataset=train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJ0-yJ4ySkC7"
   },
   "source": [
    "## Build experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "d88c5c8e210c4da0b9c2ee7f89b389dc",
      "96de69d45978498a9fad654c23241c58",
      "1ed5fde994824de5a57ea2bba1f9624c",
      "577e11275b7944bfb1144f73106e8e53",
      "6fb1d8b513ea49b48983f3790ff4ff15",
      "0957b5a407f34cf49d0618f84bdd10df",
      "4da01b3d4c69442e8e6a03b40095bbb6",
      "cede4624148a42839860d8ea571c7e21",
      "90cf42fa8cf24e98a8f219b805a2c9f9",
      "44b1b854d7a74100a560dff9047989ed",
      "e150f51034f14522a28cfcb95a514592"
     ]
    },
    "id": "SZs_er2r_1DM",
    "outputId": "9a0e0ebd-7d45-42b6-a2c9-06dcc31cd6fc"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[32m2023-11-15 11:54:53.155\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mExperiment building has started\u001b[0m\n",
      "\u001b[32m2023-11-15 11:54:53.161\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mConfig:\n",
      "{\n",
      "  \"experiment_key\": \"base\",\n",
      "  \"save_safetensors\": true,\n",
      "  \"max_shard_size\": \"10GB\",\n",
      "  \"local_rank\": 0,\n",
      "  \"use_gradient_checkpointing\": true,\n",
      "  \"trainer_key\": \"lm\",\n",
      "  \"force_fp32\": false,\n",
      "  \"force_fp16\": false,\n",
      "  \"from_gptq\": false,\n",
      "  \"huggingface_hub_token\": null,\n",
      "  \"deepspeed_stage\": 0,\n",
      "  \"deepspeed_config_path\": null,\n",
      "  \"fsdp_strategy\": \"\",\n",
      "  \"fsdp_offload\": true,\n",
      "  \"seed\": 42,\n",
      "  \"stabilize\": false,\n",
      "  \"path_to_env_file\": \"./.env\",\n",
      "  \"prepare_dataset\": true,\n",
      "  \"lora_hub_model_id\": null,\n",
      "  \"lora_model_local_path\": null,\n",
      "  \"fused_model_local_path\": null,\n",
      "  \"fuse_after_training\": false,\n",
      "  \"quantization_dataset_id\": null,\n",
      "  \"quantization_max_samples\": 1024,\n",
      "  \"quantized_model_path\": \"./quantized_model/\",\n",
      "  \"quantized_hub_model_id\": null,\n",
      "  \"quantized_hub_private_repo\": true,\n",
      "  \"dataset_key\": \"soda\",\n",
      "  \"train_local_path_to_data\": \"./train.jsonl\",\n",
      "  \"eval_local_path_to_data\": null,\n",
      "  \"shuffle\": true,\n",
      "  \"max_eval_samples\": 1000,\n",
      "  \"add_eval_to_train_if_no_path\": false,\n",
      "  \"tokenizer_name_or_path\": null,\n",
      "  \"tokenizer_use_fast\": null,\n",
      "  \"tokenizer_padding_side\": null,\n",
      "  \"collator_key\": \"lm\",\n",
      "  \"max_length\": 2048,\n",
      "  \"model_name_or_path\": \"bn22/Mistral-7B-v0.1-sharded\",\n",
      "  \"push_to_hub_bos_add_bos_token\": false,\n",
      "  \"use_flash_attention_2\": false,\n",
      "  \"trust_remote_code\": false,\n",
      "  \"device_map\": null,\n",
      "  \"prepare_model_for_kbit_training\": true,\n",
      "  \"load_in_8bit\": false,\n",
      "  \"load_in_4bit\": true,\n",
      "  \"llm_int8_threshold\": 6.0,\n",
      "  \"llm_int8_has_fp16_weight\": true,\n",
      "  \"bnb_4bit_use_double_quant\": true,\n",
      "  \"bnb_4bit_quant_type\": \"nf4\",\n",
      "  \"bnb_quantize_after_model_init\": false,\n",
      "  \"gptq_bits\": 4,\n",
      "  \"gptq_group_size\": 128,\n",
      "  \"gptq_disable_exllama\": true,\n",
      "  \"apply_lora\": true,\n",
      "  \"lora_rank\": 8,\n",
      "  \"lora_alpha\": 32,\n",
      "  \"lora_dropout\": 0.1,\n",
      "  \"raw_lora_target_modules\": \"all\",\n",
      "  \"output_dir\": \"./outputs/\",\n",
      "  \"per_device_train_batch_size\": 2,\n",
      "  \"do_eval\": false,\n",
      "  \"per_device_eval_batch_size\": null,\n",
      "  \"gradient_accumulation_steps\": 2,\n",
      "  \"eval_accumulation_steps\": null,\n",
      "  \"eval_delay\": 0,\n",
      "  \"eval_steps\": 1000,\n",
      "  \"warmup_steps\": 5,\n",
      "  \"max_steps\": 25,\n",
      "  \"num_train_epochs\": 1,\n",
      "  \"learning_rate\": 0.0002,\n",
      "  \"max_grad_norm\": 1.0,\n",
      "  \"weight_decay\": 0.001,\n",
      "  \"label_smoothing_factor\": 0.0,\n",
      "  \"logging_steps\": 1,\n",
      "  \"save_steps\": 25,\n",
      "  \"save_total_limit\": 1,\n",
      "  \"optim\": \"paged_adamw_8bit\",\n",
      "  \"push_to_hub\": false,\n",
      "  \"hub_model_id\": \"BobaZooba/AntModel-7B-XLLM-Demo-LoRA\",\n",
      "  \"hub_private_repo\": true,\n",
      "  \"report_to_wandb\": false,\n",
      "  \"wandb_api_key\": null,\n",
      "  \"wandb_project\": \"xllm-demo\",\n",
      "  \"wandb_entity\": \"bobazooba\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2023-11-15 11:54:53.168\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mConfig saved\u001b[0m\n",
      "\u001b[32m2023-11-15 11:54:53.169\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36mwarning\u001b[0m:\u001b[36m100\u001b[0m - \u001b[33m\u001b[1mYou set hub_model_id, but push_to_hub is False\u001b[0m\n",
      "\u001b[32m2023-11-15 11:54:53.171\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mChecks passed successfully\u001b[0m\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "\u001b[32m2023-11-15 11:54:53.184\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mTraining arguments was built:\n",
      "{\n",
      "  \"output_dir\": \"./outputs/\",\n",
      "  \"overwrite_output_dir\": false,\n",
      "  \"do_train\": false,\n",
      "  \"do_eval\": false,\n",
      "  \"do_predict\": false,\n",
      "  \"evaluation_strategy\": \"no\",\n",
      "  \"prediction_loss_only\": false,\n",
      "  \"per_device_train_batch_size\": 2,\n",
      "  \"per_device_eval_batch_size\": 2,\n",
      "  \"per_gpu_train_batch_size\": null,\n",
      "  \"per_gpu_eval_batch_size\": null,\n",
      "  \"gradient_accumulation_steps\": 2,\n",
      "  \"eval_accumulation_steps\": 2,\n",
      "  \"eval_delay\": 0,\n",
      "  \"learning_rate\": 0.0002,\n",
      "  \"weight_decay\": 0.001,\n",
      "  \"adam_beta1\": 0.9,\n",
      "  \"adam_beta2\": 0.999,\n",
      "  \"adam_epsilon\": 1e-08,\n",
      "  \"max_grad_norm\": 1.0,\n",
      "  \"num_train_epochs\": 1,\n",
      "  \"max_steps\": 25,\n",
      "  \"lr_scheduler_type\": \"linear\",\n",
      "  \"warmup_ratio\": 0.0,\n",
      "  \"warmup_steps\": 5,\n",
      "  \"log_level\": \"info\",\n",
      "  \"log_level_replica\": \"warning\",\n",
      "  \"log_on_each_node\": true,\n",
      "  \"logging_dir\": \"./outputs/runs/Nov15_11-54-53_1edeb6ba1b81\",\n",
      "  \"logging_strategy\": \"steps\",\n",
      "  \"logging_first_step\": true,\n",
      "  \"logging_steps\": 1,\n",
      "  \"logging_nan_inf_filter\": true,\n",
      "  \"save_strategy\": \"steps\",\n",
      "  \"save_steps\": 25,\n",
      "  \"save_total_limit\": 1,\n",
      "  \"save_safetensors\": true,\n",
      "  \"save_on_each_node\": false,\n",
      "  \"no_cuda\": false,\n",
      "  \"use_cpu\": false,\n",
      "  \"use_mps_device\": false,\n",
      "  \"seed\": 42,\n",
      "  \"data_seed\": 42,\n",
      "  \"jit_mode_eval\": false,\n",
      "  \"use_ipex\": false,\n",
      "  \"bf16\": false,\n",
      "  \"fp16\": true,\n",
      "  \"fp16_opt_level\": \"O1\",\n",
      "  \"half_precision_backend\": \"auto\",\n",
      "  \"bf16_full_eval\": false,\n",
      "  \"fp16_full_eval\": false,\n",
      "  \"tf32\": null,\n",
      "  \"local_rank\": 0,\n",
      "  \"ddp_backend\": null,\n",
      "  \"tpu_num_cores\": null,\n",
      "  \"tpu_metrics_debug\": false,\n",
      "  \"debug\": [],\n",
      "  \"dataloader_drop_last\": false,\n",
      "  \"eval_steps\": 1000,\n",
      "  \"dataloader_num_workers\": 0,\n",
      "  \"past_index\": -1,\n",
      "  \"run_name\": \"./outputs/\",\n",
      "  \"disable_tqdm\": false,\n",
      "  \"remove_unused_columns\": false,\n",
      "  \"label_names\": null,\n",
      "  \"load_best_model_at_end\": false,\n",
      "  \"metric_for_best_model\": \"loss\",\n",
      "  \"greater_is_better\": false,\n",
      "  \"ignore_data_skip\": false,\n",
      "  \"fsdp\": [],\n",
      "  \"fsdp_min_num_params\": 0,\n",
      "  \"fsdp_config\": {\n",
      "    \"min_num_params\": 0,\n",
      "    \"xla\": false,\n",
      "    \"xla_fsdp_grad_ckpt\": false\n",
      "  },\n",
      "  \"fsdp_transformer_layer_cls_to_wrap\": null,\n",
      "  \"deepspeed\": null,\n",
      "  \"label_smoothing_factor\": 0.0,\n",
      "  \"optim\": \"paged_adamw_8bit\",\n",
      "  \"optim_args\": null,\n",
      "  \"adafactor\": false,\n",
      "  \"group_by_length\": false,\n",
      "  \"length_column_name\": \"length\",\n",
      "  \"report_to\": [\n",
      "    \"tensorboard\"\n",
      "  ],\n",
      "  \"ddp_find_unused_parameters\": null,\n",
      "  \"ddp_bucket_cap_mb\": null,\n",
      "  \"ddp_broadcast_buffers\": null,\n",
      "  \"dataloader_pin_memory\": true,\n",
      "  \"skip_memory_metrics\": true,\n",
      "  \"use_legacy_prediction_loop\": false,\n",
      "  \"push_to_hub\": false,\n",
      "  \"resume_from_checkpoint\": null,\n",
      "  \"hub_model_id\": \"BobaZooba/AntModel-7B-XLLM-Demo-LoRA\",\n",
      "  \"hub_strategy\": \"checkpoint\",\n",
      "  \"hub_token\": \"<HUB_TOKEN>\",\n",
      "  \"hub_private_repo\": true,\n",
      "  \"hub_always_push\": false,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"gradient_checkpointing_kwargs\": null,\n",
      "  \"include_inputs_for_metrics\": false,\n",
      "  \"fp16_backend\": \"auto\",\n",
      "  \"push_to_hub_model_id\": null,\n",
      "  \"push_to_hub_organization\": null,\n",
      "  \"push_to_hub_token\": \"<PUSH_TO_HUB_TOKEN>\",\n",
      "  \"mp_parameters\": \"\",\n",
      "  \"auto_find_batch_size\": false,\n",
      "  \"full_determinism\": false,\n",
      "  \"torchdynamo\": null,\n",
      "  \"ray_scope\": \"last\",\n",
      "  \"ddp_timeout\": 1800,\n",
      "  \"torch_compile\": false,\n",
      "  \"torch_compile_backend\": null,\n",
      "  \"torch_compile_mode\": null,\n",
      "  \"dispatch_batches\": null,\n",
      "  \"split_batches\": false,\n",
      "  \"include_tokens_per_second\": false,\n",
      "  \"neftune_noise_alpha\": null\n",
      "}\u001b[0m\n",
      "\u001b[32m2023-11-15 11:54:53.188\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mEval dataset is None\u001b[0m\n",
      "\u001b[32m2023-11-15 11:54:53.468\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mTokenizer pad token set to eos token\u001b[0m\n",
      "\u001b[32m2023-11-15 11:54:53.473\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mTokenizer bn22/Mistral-7B-v0.1-sharded was built\u001b[0m\n",
      "\u001b[32m2023-11-15 11:54:53.475\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mCollator LMCollator was built\u001b[0m\n",
      "\u001b[32m2023-11-15 11:54:53.481\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mQuantization config was built:\n",
      "{\n",
      "  \"bnb_4bit_compute_dtype\": \"float16\",\n",
      "  \"bnb_4bit_quant_type\": \"nf4\",\n",
      "  \"bnb_4bit_use_double_quant\": true,\n",
      "  \"llm_int8_has_fp16_weight\": true,\n",
      "  \"load_in_4bit\": true\n",
      "}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/11 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d88c5c8e210c4da0b9c2ee7f89b389dc"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[32m2023-11-15 11:56:27.100\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mModel prepared for kbit training. Gradient checkpointing: True\u001b[0m\n",
      "\u001b[32m2023-11-15 11:56:27.105\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mModel bn22/Mistral-7B-v0.1-sharded was built\u001b[0m\n",
      "\u001b[32m2023-11-15 11:56:27.807\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mLoRA applied to the model bn22/Mistral-7B-v0.1-sharded\u001b[0m\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using auto half precision backend\n",
      "\u001b[32m2023-11-15 11:56:27.837\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mTrainer LMTrainer was built\u001b[0m\n",
      "\u001b[32m2023-11-15 11:56:27.839\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mExperiment built successfully\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "experiment.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "YYv3yekF_0_8",
    "outputId": "a85ff4ca-4706-4b1c-8ee4-1395a0e32d15"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[32m2023-11-15 11:56:27.851\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mTraining will start soon\u001b[0m\n",
      "***** Running training *****\n",
      "  Num examples = 160,800\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 25\n",
      "  Number of trainable parameters = 20,971,520\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 02:47, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.949200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.977800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.090800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.045600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.228800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.752100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.745500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.645800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.620500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.643600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.777400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.724800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.643500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.675400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.489100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.446700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.510700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.586000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.419400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.517700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.647900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.425800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.703200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.737300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.711000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Saving model checkpoint to ./outputs/checkpoint-25\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m2023-11-15 11:59:22.791\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mTraining end\u001b[0m\n",
      "\u001b[32m2023-11-15 11:59:22.793\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxllm.utils.logger\u001b[0m:\u001b[36minfo\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mModel saved to ./outputs/\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "experiment.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzJvZpySyLvN"
   },
   "source": [
    "# After training steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "bP6wy-T8yNm5"
   },
   "outputs": [],
   "source": [
    "# # Fuse LoRA weights\n",
    "# experiment.fuse_lora()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGr6lbtRwgAc"
   },
   "source": [
    "### Or push LoRA weights to HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "CBwXhM0M_0jx"
   },
   "outputs": [],
   "source": [
    "# # Push to hub\n",
    "# experiment.push_to_hub(\n",
    "#     repo_id=\"BobaZooba/AntModel-7B-XLLM-Demo\",\n",
    "#     private=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pl2QiIlGj7r2"
   },
   "source": [
    "# ðŸŽ‰ You are awesome!\n",
    "\n",
    "## Now you know how to prototype models using `xllm`\n",
    "\n",
    "### Explore more examples at Xâ€”LLM repo\n",
    "\n",
    "https://github.com/BobaZooba/xllm\n",
    "\n",
    "Useful materials:\n",
    "\n",
    "- [Xâ€”LLM Repo](https://github.com/BobaZooba/xllm): main repo of the `xllm` library\n",
    "- [Quickstart](https://github.com/KompleteAI/xllm/tree/docs-v1#quickstart-): basics of `xllm`\n",
    "- [Examples](https://github.com/BobaZooba/xllm/examples): minimal examples of using `xllm`\n",
    "- [Guide](https://github.com/BobaZooba/xllm/blob/main/GUIDE.md): here, we go into detail about everything the library can\n",
    "  do\n",
    "- [Demo project](https://github.com/BobaZooba/xllm-demo): here's a minimal step-by-step example of how to use Xâ€”LLM and fit it\n",
    "  into your own project\n",
    "- [WeatherGPT](https://github.com/BobaZooba/wgpt): this repository features an example of how to utilize the xllm library. Included is a solution for a common type of assessment given to LLM engineers, who typically earn between $120,000 to $140,000 annually\n",
    "- [Shurale](https://github.com/BobaZooba/shurale): project with the finetuned 7B Mistal model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oz4LrVcZlE6P"
   },
   "source": [
    "## Tale Quest\n",
    "\n",
    "`Tale Quest` is my personal project which was built using `xllm` and `Shurale`. It's an interactive text-based game\n",
    "in `Telegram` with dynamic AI characters, offering infinite scenarios\n",
    "\n",
    "You will get into exciting journeys and complete fascinating quests. Chat\n",
    "with `George Orwell`, `Tech Entrepreneur`, `Young Wizard`, `Noir Detective`, `Femme Fatale` and many more\n",
    "\n",
    "Try it now: [https://t.me/talequestbot](https://t.me/TaleQuestBot?start=Z2g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "udE7qvGJkUus"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "d88c5c8e210c4da0b9c2ee7f89b389dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_96de69d45978498a9fad654c23241c58",
       "IPY_MODEL_1ed5fde994824de5a57ea2bba1f9624c",
       "IPY_MODEL_577e11275b7944bfb1144f73106e8e53"
      ],
      "layout": "IPY_MODEL_6fb1d8b513ea49b48983f3790ff4ff15"
     }
    },
    "96de69d45978498a9fad654c23241c58": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0957b5a407f34cf49d0618f84bdd10df",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_4da01b3d4c69442e8e6a03b40095bbb6",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "1ed5fde994824de5a57ea2bba1f9624c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cede4624148a42839860d8ea571c7e21",
      "max": 11,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_90cf42fa8cf24e98a8f219b805a2c9f9",
      "value": 11
     }
    },
    "577e11275b7944bfb1144f73106e8e53": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_44b1b854d7a74100a560dff9047989ed",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_e150f51034f14522a28cfcb95a514592",
      "value": " 11/11 [01:29&lt;00:00,  5.56s/it]"
     }
    },
    "6fb1d8b513ea49b48983f3790ff4ff15": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0957b5a407f34cf49d0618f84bdd10df": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4da01b3d4c69442e8e6a03b40095bbb6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cede4624148a42839860d8ea571c7e21": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "90cf42fa8cf24e98a8f219b805a2c9f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "44b1b854d7a74100a560dff9047989ed": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e150f51034f14522a28cfcb95a514592": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
